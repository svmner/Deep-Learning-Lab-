{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567b7521-4344-4275-b902-07814ae987d9",
   "metadata": {},
   "source": [
    "## Sample Testing Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591fa0ec-ddb8-49e9-81fc-9f240bba88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4815, 0.2694, 0.2842], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(3,requires_grad = True)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dd0e1-d5ba-4d48-b659-2405cd247296",
   "metadata": {},
   "source": [
    "## Simple Graph Relating x,y,z,alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb88cd28-5a8b-4629-bbef-36dff566fd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor(3.5000, requires_grad=True)\n",
      "y = x*x:  tensor(12.2500, grad_fn=<MulBackward0>)\n",
      "z = 2*y +3:  tensor(27.5000, grad_fn=<AddBackward0>)\n",
      "alpha = 3*z + 10:  tensor(92.5000, grad_fn=<AddBackward0>)\n",
      "Working out gradients\n",
      "Gradient at x = 3.5:  tensor(42.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.5,requires_grad = True)\n",
    "y = x*x\n",
    "z = 2*y + 3\n",
    "alpha = 3*z + 10\n",
    "\n",
    "print(\"x: \",x)\n",
    "print(\"y = x*x: \",y)\n",
    "print(\"z = 2*y +3: \",z)\n",
    "print(\"alpha = 3*z + 10: \",alpha)\n",
    "\n",
    "#Work out Gradients\n",
    "alpha.backward()\n",
    "print(\"Working out gradients\")\n",
    "\n",
    "print(\"Gradient at x = 3.5: \",x.grad)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54173b-0863-4f0b-96fa-f34ea850001b",
   "metadata": {},
   "source": [
    "## Analytical vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4e0024-c1dc-4a11-8378-3ddb00a3594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical dy/dx :  tensor([3.], grad_fn=<MulBackward0>)\n",
      "PyTorch dy/dx:  tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def gradient(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([3.5],requires_grad = True)\n",
    "y = f(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"Analytical dy/dx : \",gradient(x))\n",
    "print(\"PyTorch dy/dx: \",x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc6d24-523d-43ad-8286-5a0e79c8a19c",
   "metadata": {},
   "source": [
    "## Partial Derivative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31c632d-2d2d-4a93-ab1f-48c12167a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: tensor([3.], requires_grad=True)\n",
      "v: tensor([4.], requires_grad=True)\n",
      "f(u,v): tensor([4249.], grad_fn=<AddBackward0>)\n",
      "Partial Derivative w.r.t to u = tensor([78.])\n",
      "Partial Derivative w.r.t to v = tensor([3090.])\n",
      "Partial Deriviative w.r.t to u = tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor([3.0],requires_grad = True)\n",
    "v = torch.tensor([4.0],requires_grad = True)\n",
    "x = torch.tensor([2.0],requires_grad = True)\n",
    "\n",
    "def f(u,v):\n",
    "    return (3*u)**2 + (4*v)**3 + 6*u*v\n",
    "\n",
    "print(f\"u: {u}\")\n",
    "print(f\"v: {v}\")\n",
    "print(f\"f(u,v): {f(u,v)}\")\n",
    "\n",
    "y = f(u,v)\n",
    "y.backward()\n",
    "\n",
    "print(f\"Partial Derivative w.r.t to u = {u.grad}\")\n",
    "print(f\"Partial Derivative w.r.t to v = {v.grad}\")\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 5\n",
    "\n",
    "z = f(x)\n",
    "z.backward()\n",
    "\n",
    "print(f\"Partial Deriviative w.r.t to x = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fcc293-7721-4328-b142-78a3b6c0c50f",
   "metadata": {},
   "source": [
    "## Gradient of Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "202e5b61-7a42-4e99-b73b-2c31e09efa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Result:  0.1049935854035065\n",
      "Autograd Result:  tensor([0.1050])\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "\n",
    "#Manual \n",
    "\n",
    "def sigmoid(x):\n",
    "    y = (1.0/(1.0+torch.exp(-x)))\n",
    "    return y\n",
    "\n",
    "def sigmoid_manual(x):\n",
    "    \n",
    "    #Forward Pass\n",
    "\n",
    "    a = -x\n",
    "    b = e**(a)\n",
    "    c = 1 + b\n",
    "    s = 1.0/c\n",
    "    \n",
    "    #Backward Pass\n",
    "\n",
    "    dsdc = (-1.0/c**2)\n",
    "    dsdb = dsdc*1\n",
    "    dsda = dsdb * (e**a)\n",
    "    dsdx = dsda * (-1)\n",
    "    \n",
    "    return dsdx\n",
    "\n",
    "\n",
    "x = 2.0\n",
    "x_tensor = torch.tensor([2.0],requires_grad = True)\n",
    "\n",
    "y = sigmoid_manual(x)\n",
    "y_tensor = sigmoid(x_tensor)\n",
    "y_tensor.backward()\n",
    "\n",
    "print(\"Manual Result: \",y)\n",
    "print(\"Autograd Result: \",x_tensor.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e190a87-57e2-4dcf-8f93-38ca4130b2a3",
   "metadata": {},
   "source": [
    "## Question 1 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97aba2c3-e3a5-4310-af2f-69dc26a74cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Results:  tensor([64.])\n",
      "Manual Results:  64.0\n"
     ]
    }
   ],
   "source": [
    "def manual(u,v):\n",
    "    #Forward Pass\n",
    "    a = u\n",
    "    b = v\n",
    "    x = 2*a + 3*b\n",
    "    y = 5*a*a + 3*b*b*b\n",
    "    z = 2*x + 3*y\n",
    "    \n",
    "    #Backward Pass\n",
    "\n",
    "    dzdx = 2\n",
    "    dzdy = 3\n",
    "    dzda = dzdx * 2 + dzdy * (10*a)\n",
    "    dzdb = dzdy * (10*a) + dzdy * (9*b*b)\n",
    "\n",
    "    return dzda,dzdb\n",
    "\n",
    "def autograd(u,v):\n",
    "    a = u\n",
    "    b = v\n",
    "    x = 2*a + 3*b\n",
    "    y = 5*a*a + 3*b*b*b\n",
    "    z = 2*x + 3*y\n",
    "    \n",
    "    return z\n",
    "\n",
    "u_tensor = torch.tensor([2.0],requires_grad = True)\n",
    "v_tensor = torch.tensor([3.0],requires_grad = True)\n",
    "\n",
    "u = 2.0\n",
    "v = 3.0\n",
    "\n",
    "y_tensor = autograd(u_tensor,v_tensor)\n",
    "y1,y2 = manual(u,v)\n",
    "\n",
    "y_tensor.backward()\n",
    "\n",
    "print(\"Autograd Results: \",u_tensor.grad)\n",
    "print(\"Manual Results: \",y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a432ddc-bd3e-49fe-a3e4-bff51f8f37b2",
   "metadata": {},
   "source": [
    "## Question 2 & 3 -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7108e75f-234c-4eb4-8411-4d9baddfe546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Result(ReLU) of da/dw:  tensor([4.])\n",
      "Manual Results(ReLU) of da/dw:  4.0\n",
      "Autograd Result(Sigmoid) of da/dw:  tensor([7.8802e-14])\n",
      "Manual Results(Sigmoid) of da/dw:  5.684341886080721e-14\n"
     ]
    }
   ],
   "source": [
    "#Error in sigmoid part somewhere\n",
    "\n",
    "def manual(d,e,f):\n",
    "\n",
    "    #Forward Pass\n",
    "    b = d\n",
    "    x = e\n",
    "    w = f\n",
    "    u = w*x\n",
    "    v = u+b\n",
    "    a = max(0,v)\n",
    "    \n",
    "    #Backward Pass\n",
    "    if v > 0:\n",
    "        dadv = 1\n",
    "    else:\n",
    "        dadv = 0\n",
    "    dadu = dadv * 1\n",
    "    dadw = dadu * (x)\n",
    "\n",
    "    return dadw\n",
    "\n",
    "def autograd(d,e,f):\n",
    "    b = d\n",
    "    x = e\n",
    "    w = f\n",
    "    u = w*x\n",
    "    v = u+b\n",
    "    a = max(0,v)\n",
    "\n",
    "    return a\n",
    "\n",
    "b = 3.0\n",
    "x = 4.0\n",
    "w = 5.0\n",
    "\n",
    "b_tensor = torch.tensor([3.0],requires_grad = True)\n",
    "x_tensor = torch.tensor([4.0],requires_grad = True)\n",
    "w_tensor = torch.tensor([5.0],requires_grad = True)\n",
    "\n",
    "y_tensor = autograd(b_tensor,x_tensor,w_tensor)\n",
    "\n",
    "y_tensor.backward()\n",
    "\n",
    "print(\"Autograd Result(ReLU) of da/dw: \",w_tensor.grad)\n",
    "print(\"Manual Results(ReLU) of da/dw: \",manual(b,x,w))\n",
    "    \n",
    "def manualsigmoid(d, e, f):\n",
    "    # Forward Pass\n",
    "    b = d\n",
    "    x = e\n",
    "    w = f\n",
    "    u = w * x\n",
    "    v = u + b\n",
    "    a = 1 / (1 + e**(-v))\n",
    "    \n",
    "    # Backward Pass\n",
    "    dadv = a * (1 - a)\n",
    "    dadu = dadv * 1\n",
    "    dadw = dadu * (x)\n",
    "\n",
    "    return dadw\n",
    "\n",
    "def autogradsigmoid(d,e,f):\n",
    "    b = d\n",
    "    x = e\n",
    "    w = f\n",
    "    u = w*x\n",
    "    v = u+b\n",
    "    a = 1 / (1 + e**(-v))\n",
    "\n",
    "    return a\n",
    "\n",
    "b_tensorsigmoid = torch.tensor([3.0],requires_grad = True)\n",
    "x_tensorsigmoid = torch.tensor([4.0],requires_grad = True)\n",
    "w_tensorsigmoid = torch.tensor([5.0],requires_grad = True)\n",
    "\n",
    "y_tensorsigmoid = autogradsigmoid(b_tensorsigmoid,x_tensorsigmoid,w_tensorsigmoid)\n",
    "y_tensorsigmoid.backward()\n",
    "\n",
    "print(\"Autograd Result(Sigmoid) of da/dw: \",w_tensorsigmoid.grad)\n",
    "print(\"Manual Results(Sigmoid) of da/dw: \",manualsigmoid(b,x,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08209c-507d-48b0-bf6f-c26455ae549c",
   "metadata": {},
   "source": [
    "## Question 4 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03eb35c6-045d-4b8a-9ab9-84e9ddc5b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Result:  tensor([-3.4193])\n",
      "Analytical Result:  -57.35441445039799\n"
     ]
    }
   ],
   "source": [
    "##Error Somewhere\n",
    "\n",
    "import math\n",
    "\n",
    "def f(x):\n",
    "    y = math.e ** ((-x**2) -(2*x) - (math.sin(x)))\n",
    "    return y\n",
    "    \n",
    "x = -0.5\n",
    "x_tensor = torch.tensor([-0.5],requires_grad = True)\n",
    "y = f(x_tensor)\n",
    "y.backward()\n",
    "\n",
    "def gradient(x):\n",
    "    y = math.e ** ((-x**2) -(2*x) - (math.sin(x)))\n",
    "    dydx = -math.e**(y) * (2*x + 2 + math.cos(x))\n",
    "    return dydx\n",
    "\n",
    "dydx = gradient(x)\n",
    "    \n",
    "print(\"PyTorch Result: \",x_tensor.grad)\n",
    "print(\"Analytical Result: \",dydx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c3b91-c072-4965-ad33-f1658ddfca07",
   "metadata": {},
   "source": [
    "## Question 5--\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cee77017-e2b7-4d19-8eed-c1e284ace71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Result:  993.0\n",
      "Manual Result:  tensor([993.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = (8*x**4) + (3*x**3) + (7*x**2) + (6*x) + 3\n",
    "    return y\n",
    "\n",
    "\n",
    "def gradient(x):\n",
    "    y = (32*x**3) + (9*x**2) + (14*x) + 6\n",
    "    return y\n",
    "\n",
    "x_tensor = torch.tensor([3.0],requires_grad = True)\n",
    "x = 3.0\n",
    "\n",
    "y = f(x_tensor)\n",
    "y.backward()\n",
    "\n",
    "print(\"Analytical Result: \",gradient(x))\n",
    "print(\"PyTorch Result: \",x_tensor.grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa28cfb-51a5-4352-8ddb-751a563b0b24",
   "metadata": {},
   "source": [
    "## Question 6--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d64d510-5fda-4d87-913e-ac1f4d7b6e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Result:  tensor([-0.1340])\n",
      "Analytical Result:  -0.13400446514695205\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def f(x, y, z):\n",
    "    result = torch.tanh(torch.log(1 + z * ((2 * x) / (torch.sin(y)))))\n",
    "    return result\n",
    "\n",
    "x_value = 2.0\n",
    "y_value = 1.0\n",
    "z_value = 0.5\n",
    "\n",
    "x_tensor = torch.tensor([x_value], requires_grad=True)\n",
    "y_tensor = torch.tensor([y_value], requires_grad=True)\n",
    "z_tensor = torch.tensor([z_value], requires_grad=True)\n",
    "\n",
    "y = f(x_tensor, y_tensor, z_tensor)\n",
    "y.backward()\n",
    "\n",
    "def manual(t,u,v):\n",
    "    #Forward Pass\n",
    "    x = t\n",
    "    y = u\n",
    "    z = v\n",
    "    a = 2*x\n",
    "    b = math.sin(y)\n",
    "    c = (a/(b+0.00000000000001))\n",
    "    d = c*z\n",
    "    e = math.log(d+1)\n",
    "    f = math.tanh(e)\n",
    "\n",
    "    #Backward Pass\n",
    "    dfde = 1 - math.tanh(e)**2\n",
    "    dfdd = dfde * (1/(d+1))\n",
    "    dfdc = dfdd * z\n",
    "    dfdb = dfdc * (-(a/(b**2)))\n",
    "    dfdy = dfdb * math.cos(y)\n",
    "\n",
    "    return dfdy\n",
    "     \n",
    "print(\"PyTorch Result: \", y_tensor.grad)\n",
    "print(\"Analytical Result: \",manual(x_value,y_value,z_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbb086-6154-4bff-a226-b07bf91084ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
