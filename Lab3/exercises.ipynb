{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d989b2b2-5250-4359-b4f2-0419a15a4a32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Solved Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1f7cc-b213-422a-8a2a-336292bfbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = torch.tensor([5.0,7.0,12.0,16.0,20.0])\n",
    "y = torch.tensor([40.0,120.0,180.0,210.0,240.0])\n",
    "\n",
    "w = torch.rand([1],requires_grad = True)\n",
    "b = torch.rand([1],requires_grad = True)\n",
    "\n",
    "learning_rate = torch.tensor([0.001])\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "\n",
    "    for j in range(len(x)):\n",
    "        a = w * x[j]\n",
    "        y_p = a + b\n",
    "        loss += (y_p-y[j])**2\n",
    "\n",
    "    loss = loss / len(x)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    print(\"The parameters are w={}, b={} and loss={}\".format(w,b,loss.item()))\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706505a-4260-4bc2-8725-9cd644f77167",
   "metadata": {},
   "source": [
    "## Question 1 - Implement Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a20da9-96a9-4ae1-ba93-df32e5984720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=0.8099613785743713, b=0.05486220493912697 and loss=8.568831443786621\n",
      "The parameters are w=0.8488085269927979, b=0.057060178369283676 and loss=1.2290359735488892\n",
      "The parameters are w=0.8634464740753174, b=0.05789680778980255 and loss=0.18682941794395447\n",
      "The parameters are w=0.8689618706703186, b=0.058220453560352325 and loss=0.038842350244522095\n",
      "The parameters are w=0.8710397481918335, b=0.05835079774260521 and loss=0.01782885193824768\n",
      "The parameters are w=0.8718222379684448, b=0.05840829759836197 and loss=0.014844950288534164\n",
      "The parameters are w=0.8721166253089905, b=0.05843834951519966 and loss=0.014421066269278526\n",
      "The parameters are w=0.8722270727157593, b=0.05845806002616882 and loss=0.014360728673636913\n",
      "The parameters are w=0.8722681999206543, b=0.058473873883485794 and loss=0.014352013356983662\n",
      "The parameters are w=0.8722832798957825, b=0.05848821997642517 and loss=0.01435064896941185\n",
      "The parameters are w=0.8722884654998779, b=0.05850201100111008 and loss=0.014350281096994877\n",
      "The parameters are w=0.8722899556159973, b=0.05851559340953827 and loss=0.014350036159157753\n",
      "The parameters are w=0.8722900748252869, b=0.058529093861579895 and loss=0.014349868521094322\n",
      "The parameters are w=0.8722896575927734, b=0.05854256451129913 and loss=0.014349674805998802\n",
      "The parameters are w=0.8722890019416809, b=0.058556023985147476 and loss=0.014349504373967648\n",
      "The parameters are w=0.8722882866859436, b=0.05856947973370552 and loss=0.014349355362355709\n",
      "The parameters are w=0.8722875118255615, b=0.05858293175697327 and loss=0.014349083416163921\n",
      "The parameters are w=0.8722867965698242, b=0.05859638378024101 and loss=0.014348926953971386\n",
      "The parameters are w=0.8722860217094421, b=0.05860983580350876 and loss=0.014348776079714298\n",
      "The parameters are w=0.8722853064537048, b=0.058623287826776505 and loss=0.014348569326102734\n",
      "The parameters are w=0.8722845315933228, b=0.05863673612475395 and loss=0.014348420314490795\n",
      "The parameters are w=0.8722837567329407, b=0.0586501844227314 and loss=0.014348241500556469\n",
      "The parameters are w=0.8722829818725586, b=0.05866363272070885 and loss=0.014348072931170464\n",
      "The parameters are w=0.8722822070121765, b=0.058677081018686295 and loss=0.014347918331623077\n",
      "The parameters are w=0.8722814917564392, b=0.05869052931666374 and loss=0.014347707852721214\n",
      "The parameters are w=0.8722807168960571, b=0.05870397761464119 and loss=0.014347560703754425\n",
      "The parameters are w=0.872279942035675, b=0.05871742591261864 and loss=0.014347321353852749\n",
      "The parameters are w=0.872279167175293, b=0.058730874210596085 and loss=0.014347156509757042\n",
      "The parameters are w=0.8722784519195557, b=0.05874432250857353 and loss=0.014346986077725887\n",
      "The parameters are w=0.8722776770591736, b=0.05875776708126068 and loss=0.014346778392791748\n",
      "The parameters are w=0.8722769618034363, b=0.05877121165394783 and loss=0.014346599578857422\n",
      "The parameters are w=0.8722761869430542, b=0.05878465622663498 and loss=0.014346426352858543\n",
      "The parameters are w=0.8722754120826721, b=0.05879810079932213 and loss=0.014346268959343433\n",
      "The parameters are w=0.8722746968269348, b=0.05881154537200928 and loss=0.014346080832183361\n",
      "The parameters are w=0.8722739219665527, b=0.058824989944696426 and loss=0.014345893636345863\n",
      "The parameters are w=0.8722732067108154, b=0.058838434517383575 and loss=0.014345699921250343\n",
      "The parameters are w=0.8722724318504333, b=0.058851875364780426 and loss=0.014345457777380943\n",
      "The parameters are w=0.8722716569900513, b=0.05886531621217728 and loss=0.014345320872962475\n",
      "The parameters are w=0.872270941734314, b=0.05887875705957413 and loss=0.014345153234899044\n",
      "The parameters are w=0.8722701668739319, b=0.05889219790697098 and loss=0.014344939962029457\n",
      "The parameters are w=0.8722693920135498, b=0.05890563875436783 and loss=0.014344785362482071\n",
      "The parameters are w=0.8722686171531677, b=0.05891907960176468 and loss=0.014344616793096066\n",
      "The parameters are w=0.8722678422927856, b=0.05893252044916153 and loss=0.014344452880322933\n",
      "The parameters are w=0.8722671270370483, b=0.05894596129655838 and loss=0.014344275929033756\n",
      "The parameters are w=0.8722663521766663, b=0.05895939841866493 and loss=0.014344073832035065\n",
      "The parameters are w=0.8722655773162842, b=0.058972835540771484 and loss=0.014343924820423126\n",
      "The parameters are w=0.8722648620605469, b=0.058986272662878036 and loss=0.014343705959618092\n",
      "The parameters are w=0.8722640872001648, b=0.05899970978498459 and loss=0.01434351783245802\n",
      "The parameters are w=0.8722633123397827, b=0.05901314690709114 and loss=0.014343357644975185\n",
      "The parameters are w=0.8722625374794006, b=0.05902658402919769 and loss=0.014343151822686195\n",
      "The parameters are w=0.8722618222236633, b=0.059040021151304245 and loss=0.014342975802719593\n",
      "The parameters are w=0.8722610473632812, b=0.0590534582734108 and loss=0.014342781156301498\n",
      "The parameters are w=0.872260332107544, b=0.05906689539551735 and loss=0.014342616312205791\n",
      "The parameters are w=0.8722595572471619, b=0.0590803287923336 and loss=0.014342459850013256\n",
      "The parameters are w=0.8722588419914246, b=0.05909376218914986 and loss=0.014342257753014565\n",
      "The parameters are w=0.8722580671310425, b=0.05910719558596611 and loss=0.014342083595693111\n",
      "The parameters are w=0.8722572922706604, b=0.059120628982782364 and loss=0.014341890811920166\n",
      "The parameters are w=0.8722565770149231, b=0.05913406237959862 and loss=0.014341705478727818\n",
      "The parameters are w=0.872255802154541, b=0.05914749205112457 and loss=0.014341523870825768\n",
      "The parameters are w=0.8722550272941589, b=0.05916092172265053 and loss=0.014341327361762524\n",
      "The parameters are w=0.8722543120384216, b=0.05917435139417648 and loss=0.014341142028570175\n",
      "The parameters are w=0.8722535371780396, b=0.05918778106570244 and loss=0.014340952970087528\n",
      "The parameters are w=0.8722527623176575, b=0.059201210737228394 and loss=0.014340778812766075\n",
      "The parameters are w=0.8722520470619202, b=0.05921464040875435 and loss=0.014340618625283241\n",
      "The parameters are w=0.8722512722015381, b=0.059228066354990005 and loss=0.014340484514832497\n",
      "The parameters are w=0.872250497341156, b=0.05924149230122566 and loss=0.014340310357511044\n",
      "The parameters are w=0.8722497224807739, b=0.05925492197275162 and loss=0.014340097084641457\n",
      "The parameters are w=0.8722489476203918, b=0.05926835164427757 and loss=0.014339937828481197\n",
      "The parameters are w=0.8722482323646545, b=0.05928178131580353 and loss=0.014339767396450043\n",
      "The parameters are w=0.8722474575042725, b=0.059295207262039185 and loss=0.014339562505483627\n",
      "The parameters are w=0.8722466826438904, b=0.05930863320827484 and loss=0.014339360408484936\n",
      "The parameters are w=0.8722459673881531, b=0.0593220591545105 and loss=0.0143391452729702\n",
      "The parameters are w=0.872245192527771, b=0.059335485100746155 and loss=0.014339009299874306\n",
      "The parameters are w=0.8722444772720337, b=0.05934891104698181 and loss=0.014338841661810875\n",
      "The parameters are w=0.8722437024116516, b=0.05936233326792717 and loss=0.014338626526296139\n",
      "The parameters are w=0.8722429275512695, b=0.05937575548887253 and loss=0.014338457956910133\n",
      "The parameters are w=0.8722422122955322, b=0.059389181435108185 and loss=0.01433826144784689\n",
      "The parameters are w=0.8722414374351501, b=0.05940260365605354 and loss=0.014338083565235138\n",
      "The parameters are w=0.8722406625747681, b=0.0594160258769989 and loss=0.014337950386106968\n",
      "The parameters are w=0.8722399473190308, b=0.05942944809794426 and loss=0.01433773897588253\n",
      "The parameters are w=0.8722391724586487, b=0.05944287031888962 and loss=0.014337562955915928\n",
      "The parameters are w=0.8722384572029114, b=0.059456292539834976 and loss=0.014337404631078243\n",
      "The parameters are w=0.8722376823425293, b=0.059469711035490036 and loss=0.01433718204498291\n",
      "The parameters are w=0.8722369074821472, b=0.059483129531145096 and loss=0.014336986467242241\n",
      "The parameters are w=0.8722361326217651, b=0.059496548026800156 and loss=0.014336790889501572\n",
      "The parameters are w=0.8722354173660278, b=0.059509966522455215 and loss=0.014336605556309223\n",
      "The parameters are w=0.8722346425056458, b=0.059523385018110275 and loss=0.014336477033793926\n",
      "The parameters are w=0.8722339272499084, b=0.059536803513765335 and loss=0.014336265623569489\n",
      "The parameters are w=0.8722331523895264, b=0.059550218284130096 and loss=0.014336088672280312\n",
      "The parameters are w=0.8722323775291443, b=0.059563636779785156 and loss=0.014335904270410538\n",
      "The parameters are w=0.8722316026687622, b=0.05957705155014992 and loss=0.014335747808218002\n",
      "The parameters are w=0.8722308874130249, b=0.05959047004580498 and loss=0.014335601590573788\n",
      "The parameters are w=0.8722301125526428, b=0.05960388481616974 and loss=0.014335406944155693\n",
      "The parameters are w=0.8722293376922607, b=0.0596172995865345 and loss=0.014335260726511478\n",
      "The parameters are w=0.8722286224365234, b=0.05963071435689926 and loss=0.014335048384964466\n",
      "The parameters are w=0.8722278475761414, b=0.05964412912726402 and loss=0.014334863983094692\n",
      "The parameters are w=0.8722270727157593, b=0.059657543897628784 and loss=0.014334708452224731\n",
      "The parameters are w=0.872226357460022, b=0.059670958667993546 and loss=0.014334517531096935\n",
      "The parameters are w=0.8722255825996399, b=0.05968436971306801 and loss=0.014334313571453094\n",
      "The parameters are w=0.8722248077392578, b=0.05969778075814247 and loss=0.014334122650325298\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = torch.tensor([12.4,14.3,14.5,14.9,16.1,16.9,16.5,15.4,17.0,17.9,18.8,20.3,22.4,19.4,15.5,16.7,17.3,18.4,19.2,17.4,19.5,19.7,21.2])\n",
    "y = torch.tensor([11.2,12.5,12.7,13.1,14.1,14.8,14.4,13.4,14.9,15.6,16.4,17.7,19.6,16.9,14.0,14.6,15.1,16.1,16.8,15.2,17.0,17.2,18.6])\n",
    "\n",
    "w = torch.rand([1],requires_grad = True)\n",
    "b = torch.rand([1],requires_grad = True)\n",
    "\n",
    "learning_rate = torch.tensor([0.001])\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "\n",
    "    for j in range(len(x)):\n",
    "        a = w * x[j]\n",
    "        y_p = a + b\n",
    "        loss += (y_p-y[j])**2\n",
    "\n",
    "    loss = loss / len(x)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    print(\"The parameters are w={}, b={} and loss={}\".format(w.item(),b.item(),loss.item()))\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e954f1-fe7a-4ab3-94b3-e358ebcdb1e0",
   "metadata": {},
   "source": [
    "## Question 2 - Use Analytical Solution to implement Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d516795-0530-41ca-a9d4-d0c9941606d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=0.7826786637306213, b=0.35188210010528564 and loss=867.6640625\n",
      "The parameters are w=0.9649138450622559, b=0.4064822793006897 and loss=830.2534790039062\n",
      "The parameters are w=1.1431766748428345, b=0.4598798453807831 and loss=794.45751953125\n",
      "The parameters are w=1.3175538778305054, b=0.5121010541915894 and loss=760.206298828125\n",
      "The parameters are w=1.4881302118301392, b=0.5631715059280396 and loss=727.4331665039062\n",
      "The parameters are w=1.6549885272979736, b=0.6131163835525513 and loss=696.0745239257812\n",
      "The parameters are w=1.8182101249694824, b=0.661960244178772 and loss=666.0692749023438\n",
      "The parameters are w=1.9778741598129272, b=0.7097270488739014 and loss=637.3590087890625\n",
      "The parameters are w=2.1340582370758057, b=0.7564403414726257 and loss=609.8876342773438\n",
      "The parameters are w=2.2868385314941406, b=0.8021231293678284 and loss=583.6019287109375\n",
      "The parameters are w=2.436289072036743, b=0.8467978239059448 and loss=558.4505615234375\n",
      "The parameters are w=2.5824825763702393, b=0.890486478805542 and loss=534.3847045898438\n",
      "The parameters are w=2.725490093231201, b=0.9332106113433838 and loss=511.357421875\n",
      "The parameters are w=2.8653810024261475, b=0.9749912619590759 and loss=489.3239440917969\n",
      "The parameters are w=3.002223491668701, b=1.015848994255066 and loss=468.2413330078125\n",
      "The parameters are w=3.1360838413238525, b=1.0558040142059326 and loss=448.0686340332031\n",
      "The parameters are w=3.2670273780822754, b=1.0948759317398071 and loss=428.7664794921875\n",
      "The parameters are w=3.3951175212860107, b=1.1330840587615967 and loss=410.29730224609375\n",
      "The parameters are w=3.520416736602783, b=1.1704472303390503 and loss=392.62530517578125\n",
      "The parameters are w=3.6429858207702637, b=1.2069838047027588 and loss=375.71588134765625\n",
      "The parameters are w=3.7628841400146484, b=1.2427119016647339 and loss=359.5362548828125\n",
      "The parameters are w=3.8801701068878174, b=1.277649164199829 and loss=344.0548400878906\n",
      "The parameters are w=3.994900703430176, b=1.3118128776550293 and loss=329.2415771484375\n",
      "The parameters are w=4.1071319580078125, b=1.3452198505401611 and loss=315.067626953125\n",
      "The parameters are w=4.216917991638184, b=1.3778866529464722 and loss=301.50531005859375\n",
      "The parameters are w=4.324312210083008, b=1.4098293781280518 and loss=288.52838134765625\n",
      "The parameters are w=4.4293670654296875, b=1.4410638809204102 and loss=276.1114196777344\n",
      "The parameters are w=4.53213357925415, b=1.4716055393218994 and loss=264.2303771972656\n",
      "The parameters are w=4.63266134262085, b=1.5014694929122925 and loss=252.86199951171875\n",
      "The parameters are w=4.73099946975708, b=1.5306706428527832 and loss=241.98428344726562\n",
      "The parameters are w=4.827195644378662, b=1.5592232942581177 and loss=231.57598876953125\n",
      "The parameters are w=4.9212965965271, b=1.5871416330337524 and loss=221.61688232421875\n",
      "The parameters are w=5.013347625732422, b=1.614439606666565 and loss=212.0875701904297\n",
      "The parameters are w=5.103394031524658, b=1.6411306858062744 and loss=202.96954345703125\n",
      "The parameters are w=5.191479206085205, b=1.667228102684021 and loss=194.2449951171875\n",
      "The parameters are w=5.277646064758301, b=1.6927447319030762 and loss=185.89697265625\n",
      "The parameters are w=5.361936569213867, b=1.7176933288574219 and loss=177.90919494628906\n",
      "The parameters are w=5.44439172744751, b=1.7420862913131714 and loss=170.26617431640625\n",
      "The parameters are w=5.525051593780518, b=1.7659357786178589 and loss=162.9529571533203\n",
      "The parameters are w=5.603954792022705, b=1.78925359249115 and loss=155.955322265625\n",
      "The parameters are w=5.681139945983887, b=1.8120514154434204 and loss=149.25973510742188\n",
      "The parameters are w=5.7566447257995605, b=1.8343404531478882 and loss=142.85305786132812\n",
      "The parameters are w=5.830505847930908, b=1.856131911277771 and loss=136.72288513183594\n",
      "The parameters are w=5.902759075164795, b=1.877436637878418 and loss=130.85723876953125\n",
      "The parameters are w=5.9734392166137695, b=1.8982652425765991 and loss=125.2447509765625\n",
      "The parameters are w=6.042580604553223, b=1.9186280965805054 and loss=119.87444305419922\n",
      "The parameters are w=6.110217094421387, b=1.9385353326797485 and loss=114.73591613769531\n",
      "The parameters are w=6.176381587982178, b=1.9579969644546509 and loss=109.81912994384766\n",
      "The parameters are w=6.241106033325195, b=1.977022647857666 and loss=105.11454010009766\n",
      "The parameters are w=6.304421901702881, b=1.995621919631958 and loss=100.61295318603516\n",
      "The parameters are w=6.366359710693359, b=2.0138041973114014 and loss=96.30561828613281\n",
      "The parameters are w=6.426949501037598, b=2.031578540802002 and loss=92.18419647216797\n",
      "The parameters are w=6.486220836639404, b=2.0489537715911865 and loss=88.2406234741211\n",
      "The parameters are w=6.54420280456543, b=2.0659384727478027 and loss=84.46721649169922\n",
      "The parameters are w=6.60092306137085, b=2.0825414657592773 and loss=80.85665130615234\n",
      "The parameters are w=6.65640926361084, b=2.0987708568573 and loss=77.40191650390625\n",
      "The parameters are w=6.710688591003418, b=2.1146347522735596 and loss=74.09625244140625\n",
      "The parameters are w=6.763786792755127, b=2.130141258239746 and loss=70.93321228027344\n",
      "The parameters are w=6.815730094909668, b=2.1452982425689697 and loss=67.90672302246094\n",
      "The parameters are w=6.866543769836426, b=2.1601133346557617 and loss=65.01080322265625\n",
      "The parameters are w=6.916252136230469, b=2.174593925476074 and loss=62.23983383178711\n",
      "The parameters are w=6.964879512786865, b=2.1887471675872803 and loss=59.58845901489258\n",
      "The parameters are w=7.012449264526367, b=2.202580451965332 and loss=57.051509857177734\n",
      "The parameters are w=7.058984756469727, b=2.2161006927490234 and loss=54.62403869628906\n",
      "The parameters are w=7.104508399963379, b=2.2293145656585693 and loss=52.301292419433594\n",
      "The parameters are w=7.149042129516602, b=2.2422289848327637 and loss=50.07878875732422\n",
      "The parameters are w=7.192607879638672, b=2.254850387573242 and loss=47.95219039916992\n",
      "The parameters are w=7.235226631164551, b=2.2671849727630615 and loss=45.9173583984375\n",
      "The parameters are w=7.276918888092041, b=2.2792391777038574 and loss=43.97032928466797\n",
      "The parameters are w=7.317705154418945, b=2.2910192012786865 and loss=42.10731506347656\n",
      "The parameters are w=7.35760498046875, b=2.3025310039520264 and loss=40.32469177246094\n",
      "The parameters are w=7.396637916564941, b=2.3137803077697754 and loss=38.61899185180664\n",
      "The parameters are w=7.4348225593566895, b=2.324772834777832 and loss=36.98687744140625\n",
      "The parameters are w=7.472177505493164, b=2.3355143070220947 and loss=35.42518615722656\n",
      "The parameters are w=7.508720874786377, b=2.346010208129883 and loss=33.93090057373047\n",
      "The parameters are w=7.544470310211182, b=2.3562657833099365 and loss=32.50107955932617\n",
      "The parameters are w=7.579443454742432, b=2.366286516189575 and loss=31.132980346679688\n",
      "The parameters are w=7.613656997680664, b=2.376077175140381 and loss=29.82387924194336\n",
      "The parameters are w=7.647127628326416, b=2.3856430053710938 and loss=28.571269989013672\n",
      "The parameters are w=7.679871082305908, b=2.394989013671875 and loss=27.372711181640625\n",
      "The parameters are w=7.7119035720825195, b=2.4041197299957275 and loss=26.22587013244629\n",
      "The parameters are w=7.743240833282471, b=2.4130401611328125 and loss=25.12848663330078\n",
      "The parameters are w=7.773897647857666, b=2.4217545986175537 and loss=24.078495025634766\n",
      "The parameters are w=7.803889274597168, b=2.430267810821533 and loss=23.07377052307129\n",
      "The parameters are w=7.833230018615723, b=2.4385838508605957 and loss=22.112403869628906\n",
      "The parameters are w=7.861933708190918, b=2.446707248687744 and loss=21.192516326904297\n",
      "The parameters are w=7.8900146484375, b=2.4546422958374023 and loss=20.31230926513672\n",
      "The parameters are w=7.917486667633057, b=2.462392807006836 and loss=19.470088958740234\n",
      "The parameters are w=7.944362640380859, b=2.4699630737304688 and loss=18.664199829101562\n",
      "The parameters are w=7.97065544128418, b=2.4773569107055664 and loss=17.893077850341797\n",
      "The parameters are w=7.996378421783447, b=2.4845783710479736 and loss=17.15523338317871\n",
      "The parameters are w=8.021543502807617, b=2.491631031036377 and loss=16.44919204711914\n",
      "The parameters are w=8.046162605285645, b=2.498518466949463 and loss=15.773628234863281\n",
      "The parameters are w=8.0702486038208, b=2.505244493484497 and loss=15.127201080322266\n",
      "The parameters are w=8.093811988830566, b=2.511812448501587 and loss=14.508655548095703\n",
      "The parameters are w=8.116865158081055, b=2.518225908279419 and loss=13.916799545288086\n",
      "The parameters are w=8.139418601989746, b=2.5244882106781006 and loss=13.350471496582031\n",
      "The parameters are w=8.161483764648438, b=2.5306026935577393 and loss=12.808585166931152\n",
      "The parameters are w=8.183070182800293, b=2.5365726947784424 and loss=12.290050506591797\n",
      "The parameters are w=8.20418930053711, b=2.542401075363159 and loss=11.79389762878418\n",
      "The parameters are w=8.22485065460205, b=2.548091173171997 and loss=11.319141387939453\n",
      "The parameters are w=8.245064735412598, b=2.5536458492279053 and loss=10.864862442016602\n",
      "The parameters are w=8.26484203338623, b=2.559068202972412 and loss=10.430185317993164\n",
      "The parameters are w=8.284191131591797, b=2.564361095428467 and loss=10.014242172241211\n",
      "The parameters are w=8.303121566772461, b=2.5695271492004395 and loss=9.61622428894043\n",
      "The parameters are w=8.32164192199707, b=2.5745694637298584 and loss=9.235384941101074\n",
      "The parameters are w=8.339761734008789, b=2.5794904232025146 and loss=8.87096881866455\n",
      "The parameters are w=8.357489585876465, b=2.5842928886413574 and loss=8.522269248962402\n",
      "The parameters are w=8.374834060668945, b=2.588979482650757 and loss=8.188604354858398\n",
      "The parameters are w=8.391803741455078, b=2.593552589416504 and loss=7.8693366050720215\n",
      "The parameters are w=8.408406257629395, b=2.5980145931243896 and loss=7.563821792602539\n",
      "The parameters are w=8.424650192260742, b=2.602368116378784 and loss=7.271482467651367\n",
      "The parameters are w=8.440543174743652, b=2.6066155433654785 and loss=6.9917521476745605\n",
      "The parameters are w=8.456092834472656, b=2.6107590198516846 and loss=6.724069118499756\n",
      "The parameters are w=8.471306800842285, b=2.6148009300231934 and loss=6.467935562133789\n",
      "The parameters are w=8.486191749572754, b=2.618743419647217 and loss=6.2228474617004395\n",
      "The parameters are w=8.500755310058594, b=2.622588872909546 and loss=5.988317489624023\n",
      "The parameters are w=8.515005111694336, b=2.6263391971588135 and loss=5.7639031410217285\n",
      "The parameters are w=8.528946876525879, b=2.6299965381622314 and loss=5.549155235290527\n",
      "The parameters are w=8.542588233947754, b=2.6335628032684326 and loss=5.343664646148682\n",
      "The parameters are w=8.55593490600586, b=2.637040138244629 and loss=5.147027492523193\n",
      "The parameters are w=8.56899356842041, b=2.640430450439453 and loss=4.958878040313721\n",
      "The parameters are w=8.581770896911621, b=2.643735647201538 and loss=4.778829574584961\n",
      "The parameters are w=8.59427261352539, b=2.6469576358795166 and loss=4.606544017791748\n",
      "The parameters are w=8.606505393981934, b=2.6500980854034424 and loss=4.441675186157227\n",
      "The parameters are w=8.618474960327148, b=2.6531589031219482 and loss=4.283910751342773\n",
      "The parameters are w=8.630186080932617, b=2.656141757965088 and loss=4.132930278778076\n",
      "The parameters are w=8.641645431518555, b=2.659048318862915 and loss=3.98846697807312\n",
      "The parameters are w=8.652857780456543, b=2.6618802547454834 and loss=3.850222110748291\n",
      "The parameters are w=8.663829803466797, b=2.6646392345428467 and loss=3.717939853668213\n",
      "The parameters are w=8.674565315246582, b=2.6673269271850586 and loss=3.5913429260253906\n",
      "The parameters are w=8.685070037841797, b=2.6699447631835938 and loss=3.470195770263672\n",
      "The parameters are w=8.695348739624023, b=2.672494411468506 and loss=3.3542709350585938\n",
      "The parameters are w=8.70540714263916, b=2.6749773025512695 and loss=3.243340015411377\n",
      "The parameters are w=8.715249061584473, b=2.6773948669433594 and loss=3.137173652648926\n",
      "The parameters are w=8.724879264831543, b=2.67974853515625 and loss=3.0355796813964844\n",
      "The parameters are w=8.73430347442627, b=2.682039737701416 and loss=2.938363552093506\n",
      "The parameters are w=8.743525505065918, b=2.684269905090332 and loss=2.845320224761963\n",
      "The parameters are w=8.752549171447754, b=2.6864402294158936 and loss=2.756288766860962\n",
      "The parameters are w=8.76137924194336, b=2.688552141189575 and loss=2.671085834503174\n",
      "The parameters are w=8.770020484924316, b=2.6906068325042725 and loss=2.589545965194702\n",
      "The parameters are w=8.77847671508789, b=2.692605495452881 and loss=2.51151704788208\n",
      "The parameters are w=8.786751747131348, b=2.694549322128296 and loss=2.4368369579315186\n",
      "The parameters are w=8.794849395751953, b=2.696439743041992 and loss=2.3653652667999268\n",
      "The parameters are w=8.802773475646973, b=2.698277711868286 and loss=2.296967029571533\n",
      "The parameters are w=8.810528755187988, b=2.7000644207000732 and loss=2.231520891189575\n",
      "The parameters are w=8.81811809539795, b=2.701801061630249 and loss=2.168869733810425\n",
      "The parameters are w=8.825545310974121, b=2.703488826751709 and loss=2.1089181900024414\n",
      "The parameters are w=8.832813262939453, b=2.7051286697387695 and loss=2.051542282104492\n",
      "The parameters are w=8.839925765991211, b=2.706721544265747 and loss=1.9966295957565308\n",
      "The parameters are w=8.84688663482666, b=2.708268642425537 and loss=1.9440752267837524\n",
      "The parameters are w=8.853699684143066, b=2.709770679473877 and loss=1.8937816619873047\n",
      "The parameters are w=8.860366821289062, b=2.711228847503662 and loss=1.8456361293792725\n",
      "The parameters are w=8.866891860961914, b=2.712644100189209 and loss=1.7995644807815552\n",
      "The parameters are w=8.873278617858887, b=2.714017391204834 and loss=1.7554666996002197\n",
      "The parameters are w=8.879528999328613, b=2.7153496742248535 and loss=1.7132552862167358\n",
      "The parameters are w=8.885645866394043, b=2.716641902923584 and loss=1.672850251197815\n",
      "The parameters are w=8.891633033752441, b=2.7178947925567627 and loss=1.634190559387207\n",
      "The parameters are w=8.897493362426758, b=2.719109296798706 and loss=1.5971757173538208\n",
      "The parameters are w=8.903228759765625, b=2.7202861309051514 and loss=1.561748743057251\n",
      "The parameters are w=8.908842086791992, b=2.721426248550415 and loss=1.5278363227844238\n",
      "The parameters are w=8.914337158203125, b=2.7225303649902344 and loss=1.4953792095184326\n",
      "The parameters are w=8.91971492767334, b=2.7235991954803467 and loss=1.4643093347549438\n",
      "The parameters are w=8.924979209899902, b=2.7246336936950684 and loss=1.4345703125\n",
      "The parameters are w=8.930131912231445, b=2.7256345748901367 and loss=1.4061009883880615\n",
      "The parameters are w=8.935175895690918, b=2.726602554321289 and loss=1.3788435459136963\n",
      "The parameters are w=8.940113067626953, b=2.7275383472442627 and loss=1.3527495861053467\n",
      "The parameters are w=8.944945335388184, b=2.728442668914795 and loss=1.3277744054794312\n",
      "The parameters are w=8.949675559997559, b=2.729315996170044 and loss=1.3038654327392578\n",
      "The parameters are w=8.954306602478027, b=2.730159282684326 and loss=1.2809756994247437\n",
      "The parameters are w=8.958839416503906, b=2.7309730052948 and loss=1.259058952331543\n",
      "The parameters are w=8.963276863098145, b=2.7317581176757812 and loss=1.2380790710449219\n",
      "The parameters are w=8.967620849609375, b=2.7325148582458496 and loss=1.2179899215698242\n",
      "The parameters are w=8.97187328338623, b=2.7332441806793213 and loss=1.198758602142334\n",
      "The parameters are w=8.976036071777344, b=2.7339465618133545 and loss=1.1803385019302368\n",
      "The parameters are w=8.980112075805664, b=2.7346224784851074 and loss=1.162706971168518\n",
      "The parameters are w=8.984102249145508, b=2.7352726459503174 and loss=1.1458228826522827\n",
      "The parameters are w=8.988008499145508, b=2.7358975410461426 and loss=1.1296569108963013\n",
      "The parameters are w=8.991832733154297, b=2.736497640609741 and loss=1.1141732931137085\n",
      "The parameters are w=8.995576858520508, b=2.7370736598968506 and loss=1.0993499755859375\n",
      "The parameters are w=8.999242782592773, b=2.737626075744629 and loss=1.085152268409729\n",
      "The parameters are w=9.002832412719727, b=2.7381553649902344 and loss=1.0715527534484863\n",
      "The parameters are w=9.006346702575684, b=2.738662004470825 and loss=1.0585280656814575\n",
      "The parameters are w=9.009787559509277, b=2.7391467094421387 and loss=1.0460548400878906\n",
      "The parameters are w=9.01315689086914, b=2.739609718322754 and loss=1.0341084003448486\n",
      "The parameters are w=9.01645565032959, b=2.740051507949829 and loss=1.0226651430130005\n",
      "The parameters are w=9.019686698913574, b=2.7404725551605225 and loss=1.0117042064666748\n",
      "The parameters are w=9.022850036621094, b=2.7408735752105713 and loss=1.0012035369873047\n",
      "The parameters are w=9.025947570800781, b=2.7412548065185547 and loss=0.9911391735076904\n",
      "The parameters are w=9.02898120880127, b=2.741616725921631 and loss=0.9815016388893127\n",
      "The parameters are w=9.031951904296875, b=2.741959571838379 and loss=0.9722710847854614\n",
      "The parameters are w=9.03486156463623, b=2.742284059524536 and loss=0.9634213447570801\n",
      "The parameters are w=9.037710189819336, b=2.7425904273986816 and loss=0.9549432992935181\n",
      "The parameters are w=9.04050064086914, b=2.7428789138793945 and loss=0.946816086769104\n",
      "The parameters are w=9.043232917785645, b=2.743150234222412 and loss=0.9390318393707275\n",
      "The parameters are w=9.04590892791748, b=2.7434046268463135 and loss=0.9315725564956665\n",
      "The parameters are w=9.048530578613281, b=2.7436423301696777 and loss=0.9244195818901062\n",
      "The parameters are w=9.051097869873047, b=2.743863821029663 and loss=0.917565107345581\n",
      "The parameters are w=9.05361270904541, b=2.7440695762634277 and loss=0.9109938144683838\n",
      "The parameters are w=9.056076049804688, b=2.744259834289551 and loss=0.9046933650970459\n",
      "Analytical Solution\n",
      "The parameters are w=1.1740000247955322, b=1.0520000457763672 and loss=757.0\n",
      "The parameters are w=1.344208002090454, b=1.1028521060943604 and loss=724.3797607421875\n",
      "The parameters are w=1.5107067823410034, b=1.1525812149047852 and loss=693.1673583984375\n",
      "The parameters are w=1.6735771894454956, b=1.2012118101119995 and loss=663.3018188476562\n",
      "The parameters are w=1.8328983783721924, b=1.2487679719924927 and loss=634.7252807617188\n",
      "The parameters are w=1.9887478351593018, b=1.2952730655670166 and loss=607.382080078125\n",
      "The parameters are w=2.1412012577056885, b=1.340749979019165 and loss=581.2188110351562\n",
      "The parameters are w=2.290332794189453, b=1.385221242904663 and loss=556.1846923828125\n",
      "The parameters are w=2.4362149238586426, b=1.428708791732788 and loss=532.23095703125\n",
      "The parameters are w=2.57891845703125, b=1.4712340831756592 and loss=509.31097412109375\n",
      "The parameters are w=2.718512773513794, b=1.5128180980682373 and loss=487.3801574707031\n",
      "The parameters are w=2.8550655841827393, b=1.5534813404083252 and loss=466.3957824707031\n",
      "The parameters are w=2.9886434078216553, b=1.593243956565857 and loss=446.3170471191406\n",
      "The parameters are w=3.1193110942840576, b=1.6321256160736084 and loss=427.1048278808594\n",
      "The parameters are w=3.2471320629119873, b=1.6701455116271973 and loss=408.7217102050781\n",
      "The parameters are w=3.37216854095459, b=1.7073224782943726 and loss=391.1319885253906\n",
      "The parameters are w=3.494481325149536, b=1.743674874305725 and loss=374.3014221191406\n",
      "The parameters are w=3.6141295433044434, b=1.7792205810546875 and loss=358.19708251953125\n",
      "The parameters are w=3.7311716079711914, b=1.8139773607254028 and loss=342.7878112792969\n",
      "The parameters are w=3.8456642627716064, b=1.8479623794555664 and loss=328.04351806640625\n",
      "The parameters are w=3.9576632976531982, b=1.8811924457550049 and loss=313.935546875\n",
      "The parameters are w=4.067223072052002, b=1.9136841297149658 and loss=300.4364318847656\n",
      "The parameters are w=4.174396514892578, b=1.945453405380249 and loss=287.51983642578125\n",
      "The parameters are w=4.27923583984375, b=1.9765161275863647 and loss=275.16070556640625\n",
      "The parameters are w=4.381792068481445, b=2.006887674331665 and loss=263.3349304199219\n",
      "The parameters are w=4.482114791870117, b=2.036583185195923 and loss=252.01954650878906\n",
      "The parameters are w=4.5802531242370605, b=2.065617322921753 and loss=241.1924591064453\n",
      "The parameters are w=4.6762542724609375, b=2.0940046310424805 and loss=230.83261108398438\n",
      "The parameters are w=4.770164966583252, b=2.1217591762542725 and loss=220.91983032226562\n",
      "The parameters are w=4.862030982971191, b=2.148894786834717 and loss=211.4348907470703\n",
      "The parameters are w=4.951897144317627, b=2.175424814224243 and loss=202.35926818847656\n",
      "The parameters are w=5.039806842803955, b=2.2013626098632812 and loss=193.67529296875\n",
      "The parameters are w=5.125802516937256, b=2.2267210483551025 and loss=185.36605834960938\n",
      "The parameters are w=5.209926128387451, b=2.2515127658843994 and loss=177.41543579101562\n",
      "The parameters are w=5.2922186851501465, b=2.275750160217285 and loss=169.80792236328125\n",
      "The parameters are w=5.372719764709473, b=2.299445390701294 and loss=162.52867126464844\n",
      "The parameters are w=5.451468467712402, b=2.3226101398468018 and loss=155.5635986328125\n",
      "The parameters are w=5.52850341796875, b=2.3452560901641846 and loss=148.89910888671875\n",
      "The parameters are w=5.6038618087768555, b=2.36739444732666 and loss=142.52220153808594\n",
      "The parameters are w=5.6775803565979, b=2.3890364170074463 and loss=136.42051696777344\n",
      "The parameters are w=5.749694347381592, b=2.4101929664611816 and loss=130.5821075439453\n",
      "The parameters are w=5.820239067077637, b=2.4308743476867676 and loss=124.99567413330078\n",
      "The parameters are w=5.889248847961426, b=2.4510910511016846 and loss=119.65032958984375\n",
      "The parameters are w=5.956757545471191, b=2.470853328704834 and loss=114.53563690185547\n",
      "The parameters are w=6.022797107696533, b=2.490171194076538 and loss=109.64164733886719\n",
      "The parameters are w=6.087399959564209, b=2.509054183959961 and loss=104.95887756347656\n",
      "The parameters are w=6.15059757232666, b=2.5275115966796875 and loss=100.47819519042969\n",
      "The parameters are w=6.212420463562012, b=2.545552968978882 and loss=96.19084930419922\n",
      "The parameters are w=6.2728986740112305, b=2.56318736076355 and loss=92.08851623535156\n",
      "The parameters are w=6.332061767578125, b=2.580423593521118 and loss=88.16322326660156\n",
      "The parameters are w=6.389937877655029, b=2.5972704887390137 and loss=84.40729522705078\n",
      "The parameters are w=6.4465556144714355, b=2.613736391067505 and loss=80.81344604492188\n",
      "The parameters are w=6.501942157745361, b=2.6298296451568604 and loss=77.37467956542969\n",
      "The parameters are w=6.556124210357666, b=2.6455583572387695 and loss=74.08431243896484\n",
      "The parameters are w=6.609128475189209, b=2.6609303951263428 and loss=70.93592834472656\n",
      "The parameters are w=6.660980224609375, b=2.6759538650512695 and loss=67.92340087890625\n",
      "The parameters are w=6.711704730987549, b=2.690636157989502 and loss=65.0408935546875\n",
      "The parameters are w=6.761326789855957, b=2.704984664916992 and loss=62.28274917602539\n",
      "The parameters are w=6.80987024307251, b=2.7190067768096924 and loss=59.64361572265625\n",
      "The parameters are w=6.857358932495117, b=2.7327096462249756 and loss=57.11836242675781\n",
      "The parameters are w=6.903815269470215, b=2.7461001873016357 and loss=54.70207214355469\n",
      "The parameters are w=6.9492621421813965, b=2.7591850757598877 and loss=52.39007568359375\n",
      "The parameters are w=6.993721961975098, b=2.7719712257385254 and loss=50.17780685424805\n",
      "The parameters are w=7.037215709686279, b=2.7844648361206055 and loss=48.061012268066406\n",
      "The parameters are w=7.0797648429870605, b=2.7966725826263428 and loss=46.03554916381836\n",
      "The parameters are w=7.121389389038086, b=2.808600664138794 and loss=44.09746170043945\n",
      "The parameters are w=7.162109851837158, b=2.8202550411224365 and loss=42.24301528930664\n",
      "The parameters are w=7.201946258544922, b=2.831641912460327 and loss=40.46860122680664\n",
      "The parameters are w=7.240917682647705, b=2.8427670001983643 and loss=38.770721435546875\n",
      "The parameters are w=7.279042720794678, b=2.8536360263824463 and loss=37.14610290527344\n",
      "The parameters are w=7.31633996963501, b=2.8642544746398926 and loss=35.591590881347656\n",
      "The parameters are w=7.352827548980713, b=2.8746278285980225 and loss=34.10414123535156\n",
      "The parameters are w=7.388523101806641, b=2.8847615718841553 and loss=32.68089294433594\n",
      "The parameters are w=7.4234442710876465, b=2.8946609497070312 and loss=31.319032669067383\n",
      "The parameters are w=7.457607269287109, b=2.9043309688568115 and loss=30.015907287597656\n",
      "The parameters are w=7.491029262542725, b=2.9137766361236572 and loss=28.76904296875\n",
      "The parameters are w=7.523725986480713, b=2.9230029582977295 and loss=27.575939178466797\n",
      "The parameters are w=7.555713653564453, b=2.9320147037506104 and loss=26.434343338012695\n",
      "The parameters are w=7.587007522583008, b=2.9408164024353027 and loss=25.341968536376953\n",
      "The parameters are w=7.617622375488281, b=2.9494128227233887 and loss=24.296730041503906\n",
      "The parameters are w=7.647573471069336, b=2.957808256149292 and loss=23.296573638916016\n",
      "The parameters are w=7.676875114440918, b=2.9660072326660156 and loss=22.339570999145508\n",
      "The parameters are w=7.705541610717773, b=2.9740140438079834 and loss=21.423860549926758\n",
      "The parameters are w=7.73358678817749, b=2.98183274269104 and loss=20.54764175415039\n",
      "The parameters are w=7.761023998260498, b=2.9894676208496094 and loss=19.709224700927734\n",
      "The parameters are w=7.787866592407227, b=2.996922492980957 and loss=18.906984329223633\n",
      "The parameters are w=7.8141279220581055, b=3.0042014122009277 and loss=18.139339447021484\n",
      "The parameters are w=7.839820384979248, b=3.011308193206787 and loss=17.404800415039062\n",
      "The parameters are w=7.864955902099609, b=3.018246650695801 and loss=16.701934814453125\n",
      "The parameters are w=7.889547348022461, b=3.0250203609466553 and loss=16.0294132232666\n",
      "The parameters are w=7.9136061668396, b=3.031633138656616 and loss=15.385876655578613\n",
      "The parameters are w=7.9371442794799805, b=3.038088321685791 and loss=14.77010726928711\n",
      "The parameters are w=7.960172653198242, b=3.044389247894287 and loss=14.180883407592773\n",
      "The parameters are w=7.982702732086182, b=3.050539493560791 and loss=13.617097854614258\n",
      "The parameters are w=8.004745483398438, b=3.056542158126831 and loss=13.077605247497559\n",
      "The parameters are w=8.026310920715332, b=3.0624005794525146 and loss=12.561382293701172\n",
      "The parameters are w=8.047410011291504, b=3.06811785697937 and loss=12.067421913146973\n",
      "The parameters are w=8.068053245544434, b=3.073697090148926 and loss=11.594761848449707\n",
      "The parameters are w=8.088250160217285, b=3.07914137840271 and loss=11.142485618591309\n",
      "The parameters are w=8.108010292053223, b=3.084453582763672 and loss=10.709705352783203\n",
      "The parameters are w=8.12734317779541, b=3.0896365642547607 and loss=10.295578002929688\n",
      "The parameters are w=8.146258354187012, b=3.094693183898926 and loss=9.899311065673828\n",
      "The parameters are w=8.164765357971191, b=3.099626302719116 and loss=9.520140647888184\n",
      "The parameters are w=8.182872772216797, b=3.104438543319702 and loss=9.157302856445312\n",
      "The parameters are w=8.20058822631836, b=3.1091325283050537 and loss=8.810091972351074\n",
      "The parameters are w=8.217921257019043, b=3.113710641860962 and loss=8.47788143157959\n",
      "The parameters are w=8.234880447387695, b=3.118175745010376 and loss=8.159968376159668\n",
      "The parameters are w=8.251473426818848, b=3.122530221939087 and loss=7.855773448944092\n",
      "The parameters are w=8.267708778381348, b=3.1267762184143066 and loss=7.564677715301514\n",
      "The parameters are w=8.283594131469727, b=3.1309163570404053 and loss=7.286131381988525\n",
      "The parameters are w=8.299137115478516, b=3.134953022003174 and loss=7.019587993621826\n",
      "The parameters are w=8.31434440612793, b=3.138888359069824 and loss=6.7645263671875\n",
      "The parameters are w=8.329224586486816, b=3.1427245140075684 and loss=6.520455837249756\n",
      "The parameters are w=8.343783378601074, b=3.146463632583618 and loss=6.2869062423706055\n",
      "The parameters are w=8.35802936553955, b=3.1501080989837646 and loss=6.06341028213501\n",
      "The parameters are w=8.371968269348145, b=3.1536598205566406 and loss=5.849545955657959\n",
      "The parameters are w=8.38560676574707, b=3.157120704650879 and loss=5.644894123077393\n",
      "The parameters are w=8.398951530456543, b=3.1604928970336914 and loss=5.449063777923584\n",
      "The parameters are w=8.412009239196777, b=3.163778305053711 and loss=5.261666774749756\n",
      "The parameters are w=8.424786567687988, b=3.166978597640991 and loss=5.082341194152832\n",
      "The parameters are w=8.437289237976074, b=3.170095920562744 and loss=4.910728931427002\n",
      "The parameters are w=8.449522972106934, b=3.1731319427490234 and loss=4.746495246887207\n",
      "The parameters are w=8.461493492126465, b=3.176088571548462 and loss=4.589353084564209\n",
      "The parameters are w=8.473207473754883, b=3.1789674758911133 and loss=4.438958644866943\n",
      "The parameters are w=8.48466968536377, b=3.1817703247070312 and loss=4.295048236846924\n",
      "The parameters are w=8.495885848999023, b=3.1844987869262695 and loss=4.157323837280273\n",
      "The parameters are w=8.506860733032227, b=3.187154531478882 and loss=4.025530815124512\n",
      "The parameters are w=8.517601013183594, b=3.1897389888763428 and loss=3.8994140625\n",
      "The parameters are w=8.52811050415039, b=3.192253828048706 and loss=3.7787067890167236\n",
      "The parameters are w=8.538394927978516, b=3.1947007179260254 and loss=3.6632039546966553\n",
      "The parameters are w=8.54845905303955, b=3.1970808506011963 and loss=3.5526580810546875\n",
      "The parameters are w=8.558307647705078, b=3.1993958950042725 and loss=3.4468700885772705\n",
      "The parameters are w=8.56794548034668, b=3.2016472816467285 and loss=3.345639228820801\n",
      "The parameters are w=8.577376365661621, b=3.20383620262146 and loss=3.24874210357666\n",
      "The parameters are w=8.5866060256958, b=3.2059643268585205 and loss=3.156032085418701\n",
      "The parameters are w=8.595638275146484, b=3.2080328464508057 and loss=3.067291736602783\n",
      "The parameters are w=8.604476928710938, b=3.210042953491211 and loss=2.9823577404022217\n",
      "The parameters are w=8.613126754760742, b=3.211996078491211 and loss=2.901083469390869\n",
      "The parameters are w=8.62159252166748, b=3.213893413543701 and loss=2.8232998847961426\n",
      "The parameters are w=8.629877090454102, b=3.215736150741577 and loss=2.7488534450531006\n",
      "The parameters are w=8.637985229492188, b=3.2175254821777344 and loss=2.6776039600372314\n",
      "The parameters are w=8.645920753479004, b=3.2192625999450684 and loss=2.6094093322753906\n",
      "The parameters are w=8.6536865234375, b=3.2209484577178955 and loss=2.5441410541534424\n",
      "The parameters are w=8.661287307739258, b=3.2225844860076904 and loss=2.4816737174987793\n",
      "The parameters are w=8.668725967407227, b=3.2241716384887695 and loss=2.4218802452087402\n",
      "The parameters are w=8.676006317138672, b=3.225710868835449 and loss=2.3646538257598877\n",
      "The parameters are w=8.68313217163086, b=3.227203369140625 and loss=2.3098769187927246\n",
      "The parameters are w=8.690106391906738, b=3.2286500930786133 and loss=2.257448673248291\n",
      "The parameters are w=8.696932792663574, b=3.2300522327423096 and loss=2.2072677612304688\n",
      "The parameters are w=8.703614234924316, b=3.231410503387451 and loss=2.159227132797241\n",
      "The parameters are w=8.710153579711914, b=3.2327260971069336 and loss=2.113246202468872\n",
      "The parameters are w=8.716553688049316, b=3.233999729156494 and loss=2.0692336559295654\n",
      "The parameters are w=8.722818374633789, b=3.235232353210449 and loss=2.0271053314208984\n",
      "The parameters are w=8.728950500488281, b=3.2364249229431152 and loss=1.9867804050445557\n",
      "The parameters are w=8.734952926635742, b=3.2375783920288086 and loss=1.9481799602508545\n",
      "The parameters are w=8.740828514099121, b=3.2386934757232666 and loss=1.9112193584442139\n",
      "The parameters are w=8.746580123901367, b=3.2397711277008057 and loss=1.8758411407470703\n",
      "The parameters are w=8.752209663391113, b=3.240812063217163 and loss=1.8419665098190308\n",
      "The parameters are w=8.757720947265625, b=3.2418172359466553 and loss=1.809549331665039\n",
      "The parameters are w=8.763115882873535, b=3.2427873611450195 and loss=1.7785013914108276\n",
      "The parameters are w=8.768396377563477, b=3.243723154067993 and loss=1.7487822771072388\n",
      "The parameters are w=8.773566246032715, b=3.2446253299713135 and loss=1.7203246355056763\n",
      "The parameters are w=8.778627395629883, b=3.2454946041107178 and loss=1.6930830478668213\n",
      "The parameters are w=8.783581733703613, b=3.2463319301605225 and loss=1.6669962406158447\n",
      "The parameters are w=8.788432121276855, b=3.2471377849578857 and loss=1.6420230865478516\n",
      "The parameters are w=8.793180465698242, b=3.247912883758545 and loss=1.6181015968322754\n",
      "The parameters are w=8.797829627990723, b=3.2486579418182373 and loss=1.5952033996582031\n",
      "The parameters are w=8.80238151550293, b=3.2493736743927 and loss=1.5732730627059937\n",
      "The parameters are w=8.806838035583496, b=3.250060558319092 and loss=1.5522692203521729\n",
      "The parameters are w=8.811201095581055, b=3.2507193088531494 and loss=1.532156229019165\n",
      "The parameters are w=8.815472602844238, b=3.2513506412506104 and loss=1.5128923654556274\n",
      "The parameters are w=8.819655418395996, b=3.251955032348633 and loss=1.4944491386413574\n",
      "The parameters are w=8.823750495910645, b=3.252533197402954 and loss=1.4767770767211914\n",
      "The parameters are w=8.827760696411133, b=3.2530856132507324 and loss=1.4598579406738281\n",
      "The parameters are w=8.831686973571777, b=3.253612995147705 and loss=1.4436466693878174\n",
      "The parameters are w=8.835531234741211, b=3.254115581512451 and loss=1.4281203746795654\n",
      "The parameters are w=8.839296340942383, b=3.254594087600708 and loss=1.4132473468780518\n",
      "The parameters are w=8.84298324584961, b=3.255049228668213 and loss=1.3989992141723633\n",
      "The parameters are w=8.846592903137207, b=3.255481243133545 and loss=1.385341763496399\n",
      "The parameters are w=8.850128173828125, b=3.2558906078338623 and loss=1.3722662925720215\n",
      "The parameters are w=8.85359001159668, b=3.2562780380249023 and loss=1.3597347736358643\n",
      "The parameters are w=8.856980323791504, b=3.2566440105438232 and loss=1.3477269411087036\n",
      "The parameters are w=8.86030101776123, b=3.256988763809204 and loss=1.3362185955047607\n",
      "The parameters are w=8.863553047180176, b=3.2573130130767822 and loss=1.325192928314209\n",
      "The parameters are w=8.866738319396973, b=3.2576169967651367 and loss=1.3146241903305054\n",
      "The parameters are w=8.869857788085938, b=3.257901430130005 and loss=1.304491639137268\n",
      "The parameters are w=8.872913360595703, b=3.258166551589966 and loss=1.2947865724563599\n",
      "The parameters are w=8.875905990600586, b=3.2584128379821777 and loss=1.2854760885238647\n",
      "The parameters are w=8.878837585449219, b=3.2586405277252197 and loss=1.2765557765960693\n",
      "The parameters are w=8.881709098815918, b=3.258850336074829 and loss=1.2679979801177979\n",
      "The parameters are w=8.884521484375, b=3.259042263031006 and loss=1.2597949504852295\n",
      "The parameters are w=8.887276649475098, b=3.2592170238494873 and loss=1.251932144165039\n",
      "The parameters are w=8.889975547790527, b=3.2593748569488525 and loss=1.244386911392212\n",
      "The parameters are w=8.892620086669922, b=3.2595162391662598 and loss=1.2371503114700317\n",
      "The parameters are w=8.895210266113281, b=3.259641408920288 and loss=1.2302098274230957\n",
      "The parameters are w=8.897747993469238, b=3.2597508430480957 and loss=1.223557710647583\n",
      "The parameters are w=8.90023422241211, b=3.2598447799682617 and loss=1.2171690464019775\n",
      "The parameters are w=8.902670860290527, b=3.2599236965179443 and loss=1.2110412120819092\n",
      "The parameters are w=8.905057907104492, b=3.2599878311157227 and loss=1.205161452293396\n",
      "The parameters are w=8.907397270202637, b=3.260037422180176 and loss=1.1995151042938232\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = torch.tensor([2.0, 4.0])\n",
    "y = torch.tensor([20.0, 40.0])\n",
    "\n",
    "w = torch.rand([1], requires_grad=True)\n",
    "b = torch.rand([1], requires_grad=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = 0.0\n",
    "\n",
    "    for j in range(len(x)):\n",
    "        a = w * x[j]\n",
    "        y_p = a + b\n",
    "        loss += (y_p - y[j]) ** 2\n",
    "\n",
    "    loss = loss / len(x)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    print(\"The parameters are w={}, b={} and loss={}\".format(w.item(), b.item(), loss.item()))\n",
    "\n",
    "def manual(x, y, w, b):\n",
    "    loss_list1 = []\n",
    "    for epoch in range(200):\n",
    "        loss = 0.0\n",
    "        wgrad = 0.0\n",
    "        bgrad = 0.0\n",
    "\n",
    "        for j in range(len(x)):\n",
    "            y_p = w * x[j] + b\n",
    "            loss += (y[j] - y_p) ** 2\n",
    "            wgrad += (y_p - y[j]) * x[j]\n",
    "            bgrad += (y_p - y[j])\n",
    "\n",
    "        loss = loss / len(x)\n",
    "        loss_list1.append(loss.item())\n",
    "\n",
    "        w -= learning_rate * wgrad * 2 / len(x)\n",
    "        b -= learning_rate * bgrad * 2 / len(x)\n",
    "\n",
    "        print(\"The parameters are w={}, b={} and loss={}\".format(w, b, loss))\n",
    "\n",
    "    return loss_list1\n",
    "\n",
    "print(\"Analytical Solution\")\n",
    "loss_list_manual = manual(x, y, 1, 1)\n",
    "\n",
    "plt.plot(loss_list, label=\"AutoGrad\")\n",
    "plt.plot(loss_list_manual, label=\"Manual\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffcdf4-8988-4963-83a2-88220afe50fb",
   "metadata": {},
   "source": [
    "## Question 3 - Implement Linear Regression by defining a user defined class RegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ace1fc25-bacf-4abc-be17-408214086dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=8.423766136169434, b=0.8676538467407227 and loss=59216.23828125\n",
      "The parameters are w=9.50338363647461, b=1.1395213603973389 and loss=28980.078125\n",
      "The parameters are w=9.732020378112793, b=1.3509176969528198 and loss=28201.140625\n",
      "The parameters are w=9.767341613769531, b=1.5481129884719849 and loss=28127.81640625\n",
      "The parameters are w=9.763612747192383, b=1.7422846555709839 and loss=28088.66796875\n",
      "The parameters are w=9.751733779907227, b=1.9356799125671387 and loss=28051.01171875\n",
      "The parameters are w=9.738146781921387, b=2.128767728805542 and loss=28013.490234375\n",
      "The parameters are w=9.724211692810059, b=2.3216469287872314 and loss=27976.041015625\n",
      "The parameters are w=9.710214614868164, b=2.514338731765747 and loss=27938.662109375\n",
      "The parameters are w=9.696215629577637, b=2.7068474292755127 and loss=27901.349609375\n",
      "The parameters are w=9.682228088378906, b=2.899174213409424 and loss=27864.115234375\n",
      "The parameters are w=9.668254852294922, b=3.0913190841674805 and loss=27826.947265625\n",
      "The parameters are w=9.654295921325684, b=3.283282518386841 and loss=27789.849609375\n",
      "The parameters are w=9.640352249145508, b=3.475064516067505 and loss=27752.82421875\n",
      "The parameters are w=9.626422882080078, b=3.6666650772094727 and loss=27715.865234375\n",
      "The parameters are w=9.612507820129395, b=3.8580844402313232 and loss=27678.978515625\n",
      "The parameters are w=9.598608016967773, b=4.049322605133057 and loss=27642.16015625\n",
      "The parameters are w=9.584722518920898, b=4.240379810333252 and loss=27605.412109375\n",
      "The parameters are w=9.570852279663086, b=4.43125581741333 and loss=27568.734375\n",
      "The parameters are w=9.55699634552002, b=4.621951103210449 and loss=27532.125\n",
      "The parameters are w=9.543155670166016, b=4.812465667724609 and loss=27495.587890625\n",
      "The parameters are w=9.529328346252441, b=5.0027995109558105 and loss=27459.115234375\n",
      "The parameters are w=9.515517234802246, b=5.192952632904053 and loss=27422.712890625\n",
      "The parameters are w=9.501720428466797, b=5.382925510406494 and loss=27386.384765625\n",
      "The parameters are w=9.487937927246094, b=5.572717666625977 and loss=27350.12109375\n",
      "The parameters are w=9.474169731140137, b=5.762329578399658 and loss=27313.927734375\n",
      "The parameters are w=9.460415840148926, b=5.951761722564697 and loss=27277.802734375\n",
      "The parameters are w=9.446677207946777, b=6.1410136222839355 and loss=27241.74609375\n",
      "The parameters are w=9.432953834533691, b=6.330085277557373 and loss=27205.755859375\n",
      "The parameters are w=9.419244766235352, b=6.518977165222168 and loss=27169.837890625\n",
      "The parameters are w=9.405550003051758, b=6.70768928527832 and loss=27133.98828125\n",
      "The parameters are w=9.391870498657227, b=6.89622163772583 and loss=27098.203125\n",
      "The parameters are w=9.378204345703125, b=7.084574222564697 and loss=27062.490234375\n",
      "The parameters are w=9.364554405212402, b=7.27274751663208 and loss=27026.84375\n",
      "The parameters are w=9.350918769836426, b=7.4607415199279785 and loss=26991.265625\n",
      "The parameters are w=9.337297439575195, b=7.648556232452393 and loss=26955.755859375\n",
      "The parameters are w=9.323689460754395, b=7.836191654205322 and loss=26920.3125\n",
      "The parameters are w=9.310097694396973, b=8.023648262023926 and loss=26884.9375\n",
      "The parameters are w=9.296520233154297, b=8.210925102233887 and loss=26849.62890625\n",
      "The parameters are w=9.282958030700684, b=8.39802360534668 and loss=26814.38671875\n",
      "The parameters are w=9.2694091796875, b=8.584942817687988 and loss=26779.21484375\n",
      "The parameters are w=9.255875587463379, b=8.771683692932129 and loss=26744.109375\n",
      "The parameters are w=9.24235725402832, b=8.958245277404785 and loss=26709.072265625\n",
      "The parameters are w=9.228852272033691, b=9.144628524780273 and loss=26674.099609375\n",
      "The parameters are w=9.215362548828125, b=9.330833435058594 and loss=26639.193359375\n",
      "The parameters are w=9.201887130737305, b=9.516860008239746 and loss=26604.359375\n",
      "The parameters are w=9.18842601776123, b=9.70270824432373 and loss=26569.587890625\n",
      "The parameters are w=9.174980163574219, b=9.888378143310547 and loss=26534.884765625\n",
      "The parameters are w=9.161548614501953, b=10.073869705200195 and loss=26500.24609375\n",
      "The parameters are w=9.14813232421875, b=10.259183883666992 and loss=26465.67578125\n",
      "The parameters are w=9.134729385375977, b=10.444319725036621 and loss=26431.16796875\n",
      "The parameters are w=9.121341705322266, b=10.629278182983398 and loss=26396.73046875\n",
      "The parameters are w=9.107969284057617, b=10.814058303833008 and loss=26362.359375\n",
      "The parameters are w=9.094610214233398, b=10.998661041259766 and loss=26328.056640625\n",
      "The parameters are w=9.081266403198242, b=11.183086395263672 and loss=26293.8125\n",
      "The parameters are w=9.067936897277832, b=11.36733341217041 and loss=26259.63671875\n",
      "The parameters are w=9.054621696472168, b=11.551403045654297 and loss=26225.53125\n",
      "The parameters are w=9.041321754455566, b=11.735296249389648 and loss=26191.48828125\n",
      "The parameters are w=9.028036117553711, b=11.919012069702148 and loss=26157.509765625\n",
      "The parameters are w=9.014764785766602, b=12.102550506591797 and loss=26123.59765625\n",
      "The parameters are w=9.001508712768555, b=12.285911560058594 and loss=26089.75390625\n",
      "The parameters are w=8.988265991210938, b=12.469096183776855 and loss=26055.97265625\n",
      "The parameters are w=8.9750394821167, b=12.652103424072266 and loss=26022.255859375\n",
      "The parameters are w=8.96182632446289, b=12.83493423461914 and loss=25988.60546875\n",
      "The parameters are w=8.948628425598145, b=13.01758861541748 and loss=25955.01953125\n",
      "The parameters are w=8.935443878173828, b=13.200065612792969 and loss=25921.5\n",
      "The parameters are w=8.922274589538574, b=13.382366180419922 and loss=25888.046875\n",
      "The parameters are w=8.909119606018066, b=13.56449031829834 and loss=25854.65234375\n",
      "The parameters are w=8.895979881286621, b=13.746438980102539 and loss=25821.326171875\n",
      "The parameters are w=8.882854461669922, b=13.928211212158203 and loss=25788.06640625\n",
      "The parameters are w=8.869743347167969, b=14.109807014465332 and loss=25754.87109375\n",
      "The parameters are w=8.856646537780762, b=14.291226387023926 and loss=25721.734375\n",
      "The parameters are w=8.843564987182617, b=14.4724702835083 and loss=25688.66796875\n",
      "The parameters are w=8.830496788024902, b=14.65353775024414 and loss=25655.662109375\n",
      "The parameters are w=8.817444801330566, b=14.834429740905762 and loss=25622.72265625\n",
      "The parameters are w=8.80440616607666, b=15.015146255493164 and loss=25589.84375\n",
      "The parameters are w=8.7913818359375, b=15.195687294006348 and loss=25557.03515625\n",
      "The parameters are w=8.778372764587402, b=15.376052856445312 and loss=25524.287109375\n",
      "The parameters are w=8.76537799835205, b=15.556241989135742 and loss=25491.599609375\n",
      "The parameters are w=8.752397537231445, b=15.73625659942627 and loss=25458.978515625\n",
      "The parameters are w=8.739431381225586, b=15.916095733642578 and loss=25426.41796875\n",
      "The parameters are w=8.726479530334473, b=16.095760345458984 and loss=25393.927734375\n",
      "The parameters are w=8.713542938232422, b=16.275249481201172 and loss=25361.49609375\n",
      "The parameters are w=8.700620651245117, b=16.45456314086914 and loss=25329.130859375\n",
      "The parameters are w=8.687712669372559, b=16.63370132446289 and loss=25296.82421875\n",
      "The parameters are w=8.674819946289062, b=16.812665939331055 and loss=25264.58203125\n",
      "The parameters are w=8.661940574645996, b=16.991455078125 and loss=25232.40234375\n",
      "The parameters are w=8.649076461791992, b=17.170068740844727 and loss=25200.2890625\n",
      "The parameters are w=8.636226654052734, b=17.348508834838867 and loss=25168.23828125\n",
      "The parameters are w=8.623391151428223, b=17.52677345275879 and loss=25136.24609375\n",
      "The parameters are w=8.610569953918457, b=17.704864501953125 and loss=25104.318359375\n",
      "The parameters are w=8.597764015197754, b=17.882780075073242 and loss=25072.453125\n",
      "The parameters are w=8.584972381591797, b=18.060522079467773 and loss=25040.650390625\n",
      "The parameters are w=8.572195053100586, b=18.23809051513672 and loss=25008.91015625\n",
      "The parameters are w=8.559432029724121, b=18.415483474731445 and loss=24977.234375\n",
      "The parameters are w=8.546683311462402, b=18.592702865600586 and loss=24945.615234375\n",
      "The parameters are w=8.533949851989746, b=18.76974868774414 and loss=24914.0625\n",
      "The parameters are w=8.52122974395752, b=18.94662094116211 and loss=24882.5703125\n",
      "The parameters are w=8.508524894714355, b=19.123319625854492 and loss=24851.140625\n",
      "The parameters are w=8.495834350585938, b=19.299842834472656 and loss=24819.771484375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor([5.0,7.0,12.0,16.0,20.0])\n",
    "y = torch.tensor([40.0,120.0,180.0,210.0,240.0])\n",
    "learning_rate = torch.tensor(0.001)\n",
    "\n",
    "class RegressionModel:\n",
    "    def __init__(self):\n",
    "        self.w = torch.rand([1],requires_grad = True)\n",
    "        self.b = torch.rand([1],requires_grad = True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.w*x + self.b\n",
    "        \n",
    "    def update(self):\n",
    "        self.w -= learning_rate*self.w.grad\n",
    "        self.b -= learning_rate*self.b.grad\n",
    "        \n",
    "    def reset_grad(self):\n",
    "        self.w.grad.zero_()\n",
    "        self.b.grad.zero_()\n",
    "\n",
    "model = RegressionModel()\n",
    "loss_list = []\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "    for j in range(len(x)):\n",
    "        y_p = model.forward(x)\n",
    "        loss += torch.norm((y[j] - y_p)**2)\n",
    "    loss = loss/len(x)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.update()\n",
    "    model.reset_grad()\n",
    "\n",
    "    print(\"The parameters are w={}, b={} and loss={}\".format(model.w.item(), model.b.item(), loss.item()))\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee8fc58-e535-433e-8d71-9d947c72a9ef",
   "metadata": {},
   "source": [
    "## Question 4 - Extend Q3 to use nn.module, use Dataset and DataLoader along with torch.optim.sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30c9966e-feb9-40b7-90aa-22c2cb03844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=14.537090301513672, b=1.8499560356140137 and loss=2798.302001953125\n",
      "The parameters are w=14.507948875427246, b=1.8923665285110474 and loss=1905.1068115234375\n",
      "The parameters are w=14.504724502563477, b=1.9363758563995361 and loss=1900.0692138671875\n",
      "The parameters are w=14.501168251037598, b=1.9802953004837036 and loss=1898.8804931640625\n",
      "The parameters are w=9.09237289428711, b=1.6823610067367554 and loss=3226.25\n",
      "The parameters are w=13.24530029296875, b=1.9854573011398315 and loss=104.09825134277344\n",
      "The parameters are w=14.506917953491211, b=2.1105642318725586 and loss=1714.659912109375\n",
      "The parameters are w=9.085489273071289, b=1.811466932296753 and loss=3227.2255859375\n",
      "The parameters are w=14.468099594116211, b=2.2247180938720703 and loss=1111.03369140625\n",
      "The parameters are w=11.845236778259277, b=1.9919118881225586 and loss=1264.031494140625\n",
      "The parameters are w=14.517223358154297, b=2.2075982093811035 and loss=1521.58740234375\n",
      "The parameters are w=13.536698341369629, b=2.242382287979126 and loss=2159.320556640625\n",
      "The parameters are w=9.46640396118164, b=2.034168243408203 and loss=2393.349609375\n",
      "The parameters are w=13.01467227935791, b=2.172295331954956 and loss=2084.20166015625\n",
      "The parameters are w=13.087262153625488, b=2.187828302383423 and loss=17.08237648010254\n",
      "The parameters are w=14.49167537689209, b=2.3217926025390625 and loss=1689.2401123046875\n",
      "The parameters are w=13.029850006103516, b=2.229235887527466 and loss=3.4014732837677\n",
      "The parameters are w=13.083343505859375, b=2.2433462142944336 and loss=16.928251266479492\n",
      "The parameters are w=12.161869049072266, b=2.112558364868164 and loss=1475.1856689453125\n",
      "The parameters are w=14.504958152770996, b=2.306931734085083 and loss=1562.4677734375\n",
      "The parameters are w=9.079338073730469, b=2.0070836544036865 and loss=3218.93408203125\n",
      "The parameters are w=14.537385940551758, b=2.401698112487793 and loss=1174.0137939453125\n",
      "The parameters are w=14.463056564331055, b=2.4403486251831055 and loss=1896.3048095703125\n",
      "The parameters are w=13.023670196533203, b=2.349202871322632 and loss=3.6073923110961914\n",
      "The parameters are w=13.76828384399414, b=2.4858925342559814 and loss=1823.40966796875\n",
      "The parameters are w=9.365781784057617, b=2.2546751499176025 and loss=2572.9541015625\n",
      "The parameters are w=13.015776634216309, b=2.3981359004974365 and loss=2119.74853515625\n",
      "The parameters are w=13.764792442321777, b=2.5349485874176025 and loss=1819.1651611328125\n",
      "The parameters are w=13.040885925292969, b=2.4939727783203125 and loss=8.974918365478516\n",
      "The parameters are w=14.467126846313477, b=2.6288633346557617 and loss=1678.099365234375\n",
      "The parameters are w=13.503700256347656, b=2.66347336769104 and loss=2123.774169921875\n",
      "The parameters are w=12.023198127746582, b=2.4976766109466553 and loss=1435.77783203125\n",
      "The parameters are w=12.380284309387207, b=2.442714214324951 and loss=1663.9246826171875\n",
      "The parameters are w=9.91889476776123, b=2.342618465423584 and loss=1560.9566650390625\n",
      "The parameters are w=12.879770278930664, b=2.444424867630005 and loss=2023.0947265625\n",
      "The parameters are w=14.472402572631836, b=2.5900797843933105 and loss=1656.28857421875\n",
      "The parameters are w=14.448232650756836, b=2.6316592693328857 and loss=1883.610107421875\n",
      "The parameters are w=9.09042739868164, b=2.3356146812438965 and loss=3156.43359375\n",
      "The parameters are w=13.206137657165527, b=2.6351757049560547 and loss=105.33263397216797\n",
      "The parameters are w=13.710309028625488, b=2.7567999362945557 and loss=1846.7628173828125\n",
      "The parameters are w=14.440563201904297, b=2.8465778827667236 and loss=1769.3515625\n",
      "The parameters are w=11.789419174194336, b=2.60990309715271 and loss=1304.73291015625\n",
      "The parameters are w=13.10116195678711, b=2.7119412422180176 and loss=36.97795104980469\n",
      "The parameters are w=9.62292194366455, b=2.542569875717163 and loss=2048.63671875\n",
      "The parameters are w=14.308222770690918, b=2.9124066829681396 and loss=1169.9012451171875\n",
      "The parameters are w=9.136159896850586, b=2.628230333328247 and loss=3021.458984375\n",
      "The parameters are w=14.379831314086914, b=3.030938148498535 and loss=1085.8115234375\n",
      "The parameters are w=14.413081169128418, b=3.0755205154418945 and loss=1862.859375\n",
      "The parameters are w=9.088737487792969, b=2.78067684173584 and loss=3110.12255859375\n",
      "The parameters are w=14.37270450592041, b=3.1853444576263428 and loss=1071.9398193359375\n",
      "The parameters are w=11.77116870880127, b=2.950463056564331 and loss=1335.0859375\n",
      "The parameters are w=13.081310272216797, b=3.051909923553467 and loss=37.69047546386719\n",
      "The parameters are w=13.032256126403809, b=3.0576536655426025 and loss=16.86190414428711\n",
      "The parameters are w=9.638106346130371, b=2.893134593963623 and loss=1989.498046875\n",
      "The parameters are w=14.271416664123535, b=3.258899211883545 and loss=1157.6546630859375\n",
      "The parameters are w=14.395371437072754, b=3.3089537620544434 and loss=1843.2535400390625\n",
      "The parameters are w=12.973731994628906, b=3.21783447265625 and loss=4.330514430999756\n",
      "The parameters are w=13.0258207321167, b=3.2304558753967285 and loss=18.391572952270508\n",
      "The parameters are w=12.07644271850586, b=3.0945465564727783 and loss=1547.467529296875\n",
      "The parameters are w=10.016645431518555, b=3.0200040340423584 and loss=1358.7464599609375\n",
      "The parameters are w=12.789587020874023, b=3.108168601989746 and loss=2055.302490234375\n",
      "The parameters are w=13.732468605041504, b=3.2544751167297363 and loss=1735.151123046875\n",
      "The parameters are w=13.563199043273926, b=3.3344156742095947 and loss=1926.11083984375\n",
      "The parameters are w=9.41734504699707, b=3.118499517440796 and loss=2383.208740234375\n",
      "The parameters are w=13.148406982421875, b=3.38991641998291 and loss=96.4781265258789\n",
      "The parameters are w=14.393400192260742, b=3.511772632598877 and loss=1679.7032470703125\n",
      "The parameters are w=14.373854637145996, b=3.5522167682647705 and loss=1857.239990234375\n",
      "The parameters are w=14.370716094970703, b=3.5936520099639893 and loss=1853.7152099609375\n",
      "The parameters are w=13.425115585327148, b=3.6265976428985596 and loss=2047.560546875\n",
      "The parameters are w=9.461915016174316, b=3.422313928604126 and loss=2267.278076171875\n",
      "The parameters are w=12.87696647644043, b=3.547600269317627 and loss=2190.59765625\n",
      "The parameters are w=13.675098419189453, b=3.6840834617614746 and loss=1730.43701171875\n",
      "The parameters are w=11.8817720413208, b=3.49595308303833 and loss=1472.986083984375\n",
      "The parameters are w=13.044928550720215, b=3.5862984657287598 and loss=36.176639556884766\n",
      "The parameters are w=9.614420890808105, b=3.4180657863616943 and loss=1984.90380859375\n",
      "The parameters are w=13.123955726623535, b=3.6734864711761475 and loss=91.0393295288086\n",
      "The parameters are w=9.57994270324707, b=3.497384786605835 and loss=2039.9766845703125\n",
      "The parameters are w=14.221834182739258, b=3.8619539737701416 and loss=1122.7310791015625\n",
      "The parameters are w=11.738245010375977, b=3.631786346435547 and loss=1399.106201171875\n",
      "The parameters are w=14.384631156921387, b=3.8432798385620117 and loss=1483.9942626953125\n",
      "The parameters are w=9.072957038879395, b=3.547462224960327 and loss=3059.7119140625\n",
      "The parameters are w=12.953919410705566, b=3.700266122817993 and loss=2275.7041015625\n",
      "The parameters are w=9.646566390991211, b=3.540074348449707 and loss=1916.9400634765625\n",
      "The parameters are w=10.966561317443848, b=3.6925952434539795 and loss=291.567626953125\n",
      "The parameters are w=10.436614990234375, b=3.7198877334594727 and loss=767.7750854492188\n",
      "The parameters are w=13.07887077331543, b=3.9139087200164795 and loss=68.66705322265625\n",
      "The parameters are w=13.606066703796387, b=4.033291339874268 and loss=1753.037109375\n",
      "The parameters are w=12.9561185836792, b=3.995408296585083 and loss=11.387754440307617\n",
      "The parameters are w=14.345624923706055, b=4.125606060028076 and loss=1643.755615234375\n",
      "The parameters are w=14.3242826461792, b=4.164980888366699 and loss=1840.4937744140625\n",
      "The parameters are w=14.321244239807129, b=4.205470085144043 and loss=1836.7352294921875\n",
      "The parameters are w=11.681047439575195, b=3.9647064208984375 and loss=1404.9957275390625\n",
      "The parameters are w=12.312356948852539, b=3.921074151992798 and loss=1823.22998046875\n",
      "The parameters are w=12.171515464782715, b=3.8312652111053467 and loss=1711.3802490234375\n",
      "The parameters are w=12.212884902954102, b=3.7527027130126953 and loss=1729.2119140625\n",
      "The parameters are w=9.939202308654785, b=3.6621553897857666 and loss=1426.8114013671875\n",
      "The parameters are w=14.396078109741211, b=3.989891290664673 and loss=1255.52978515625\n",
      "The parameters are w=11.685381889343262, b=3.7456507682800293 and loss=1380.3104248046875\n",
      "The parameters are w=10.14908218383789, b=3.7048912048339844 and loss=1122.791015625\n",
      "The parameters are w=12.690675735473633, b=3.7767832279205322 and loss=2081.8310546875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "x = torch.tensor([5.0,7.0,12.0,16.0,20.0])\n",
    "y = torch.tensor([40.0,120.0,180.0,210.0,240.0])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.Y[idx]\n",
    "\n",
    "dataset = MyDataset(x,y)\n",
    "data_loader = DataLoader(dataset,batch_size = 4,shuffle = True)\n",
    "loss_list = []\n",
    "\n",
    "learning_rate = torch.tensor([0.0001])\n",
    "\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        self.b = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "            return self.w*x + self.b\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "model = RegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.003)\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "    for i,data in enumerate(data_loader):\n",
    "        inputs,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss += loss.item()\n",
    "\n",
    "    print(\"The parameters are w={}, b={} and loss={}\".format(model.w.item(), model.b.item(), loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2c58a-fc14-4b92-95dd-e96170358084",
   "metadata": {},
   "source": [
    "## Question 5 - Use nn.Linear() to perform linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08bd4b-ae7e-4c1e-bc6e-033d03a86df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.tensor([12.4,14.3,14.5,14.9,16.1,16.9,16.5,15.4,17.0,17.9,18.8,20.3,22.4,19.4,15.5,16.7,17.3,18.4,19.2,17.4,19.5,19.7,21.2]))\n",
    "y = Variable(torch.tensor([11.2,12.5,12.7,13.1,14.1,14.8,14.4,13.4,14.9,15.6,16.4,17.7,19.6,16.9,14.0,14.6,15.1,16.1,16.8,15.2,17.0,17.2,18.6]))\n",
    "\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(23, 23)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = RegressionModel()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epochs in range(100):\n",
    "    pred_y = model(x)\n",
    "    loss = criterion(pred_y, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epochs%10 == 0:\n",
    "        print(\"The loss after epoch {} = {}\".format(epochs,loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a79866-1988-45c7-9f43-09336e5ff0be",
   "metadata": {},
   "source": [
    "## Question 6 - Implement Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86e389b0-78d3-495c-8f6c-520cbfe8e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w1=0.5405848026275635, w2=0.05630870535969734 b=0.15428341925144196 and loss=1.9386173486709595\n",
      "The parameters are w1=0.5744920969009399, w2=0.03078472800552845 b=0.15998554229736328 and loss=1.548548698425293\n",
      "The parameters are w1=0.6070821285247803, w2=0.004653804004192352 b=0.16546134650707245 and loss=1.489126205444336\n",
      "The parameters are w1=0.6516848802566528, w2=-0.012203019112348557 b=0.17579050362110138 and loss=36.713130950927734\n",
      "The parameters are w1=0.7332437634468079, w2=-0.01009361445903778 b=0.18948474526405334 and loss=116.88104248046875\n",
      "The parameters are w1=0.8077191710472107, w2=-0.01280278991907835 b=0.20176556706428528 and loss=102.18604278564453\n",
      "The parameters are w1=0.8272370100021362, w2=-0.05202918127179146 b=0.20427225530147552 and loss=0.14533531665802002\n",
      "The parameters are w1=0.8306790590286255, w2=-0.1346505731344223 b=0.20125892758369446 and loss=72.11531066894531\n",
      "The parameters are w1=0.8723317980766296, w2=-0.1482687145471573 b=0.21106791496276855 and loss=31.04427719116211\n",
      "The parameters are w1=0.9056801199913025, w2=-0.1717112511396408 b=0.21903136372566223 and loss=30.232702255249023\n",
      "The parameters are w1=0.9255006909370422, w2=-0.20585237443447113 b=0.22191216051578522 and loss=2.2845098972320557\n",
      "The parameters are w1=0.9958917498588562, w2=-0.20354017615318298 b=0.23398596048355103 and loss=84.47950744628906\n",
      "The parameters are w1=1.0577837228775024, w2=-0.2090248018503189 b=0.24419820308685303 and loss=74.34942626953125\n",
      "The parameters are w1=1.0753942728042603, w2=-0.24115946888923645 b=0.246627077460289 and loss=0.05919896066188812\n",
      "The parameters are w1=1.088093638420105, w2=-0.28012052178382874 b=0.24820438027381897 and loss=4.173187732696533\n",
      "The parameters are w1=1.0940299034118652, w2=-0.34670063853263855 b=0.24647746980190277 and loss=49.881813049316406\n",
      "The parameters are w1=1.1174737215042114, w2=-0.3688111901283264 b=0.25060930848121643 and loss=1.583361029624939\n",
      "The parameters are w1=1.1384501457214355, w2=-0.39352503418922424 b=0.2541141211986542 and loss=1.3243844509124756\n",
      "The parameters are w1=1.1686686277389526, w2=-0.403842955827713 b=0.25980547070503235 and loss=0.8149985074996948\n",
      "The parameters are w1=1.1808867454528809, w2=-0.45533376932144165 b=0.25994759798049927 and loss=36.90610122680664\n",
      "The parameters are w1=1.216446876525879, w2=-0.46301260590553284 b=0.2687342166900635 and loss=24.5479793548584\n",
      "The parameters are w1=1.2770878076553345, w2=-0.4574848413467407 b=0.27942171692848206 and loss=58.97138214111328\n",
      "The parameters are w1=1.2914632558822632, w2=-0.4860687851905823 b=0.281651109457016 and loss=1.936909794807434\n",
      "The parameters are w1=1.320790410041809, w2=-0.4981343448162079 b=0.2890896499156952 and loss=21.874473571777344\n",
      "The parameters are w1=1.345223307609558, w2=-0.5110280513763428 b=0.2936183214187622 and loss=0.34533625841140747\n",
      "The parameters are w1=1.3697481155395508, w2=-0.528806209564209 b=0.29995471239089966 and loss=20.841157913208008\n",
      "The parameters are w1=1.418475866317749, w2=-0.5312032699584961 b=0.3083590865135193 and loss=44.320823669433594\n",
      "The parameters are w1=1.436411738395691, w2=-0.5500954389572144 b=0.31147873401641846 and loss=0.08911141008138657\n",
      "The parameters are w1=1.4562221765518188, w2=-0.5712152123451233 b=0.31676551699638367 and loss=18.959897994995117\n",
      "The parameters are w1=1.4981653690338135, w2=-0.5787205100059509 b=0.32382485270500183 and loss=37.339717864990234\n",
      "The parameters are w1=1.5096790790557861, w2=-0.6041439771652222 b=0.3256257176399231 and loss=0.05828956514596939\n",
      "The parameters are w1=1.518540620803833, w2=-0.6326300501823425 b=0.3270117938518524 and loss=2.7202324867248535\n",
      "The parameters are w1=1.542368769645691, w2=-0.6452865600585938 b=0.3332720994949341 and loss=17.556730270385742\n",
      "The parameters are w1=1.545777678489685, w2=-0.6925277709960938 b=0.3322458863258362 and loss=23.642602920532227\n",
      "The parameters are w1=1.5700386762619019, w2=-0.7024393081665039 b=0.33870115876197815 and loss=17.513381958007812\n",
      "The parameters are w1=1.59055757522583, w2=-0.7123565673828125 b=0.34277501702308655 and loss=0.30341291427612305\n",
      "The parameters are w1=1.63137948513031, w2=-0.7132117748260498 b=0.35005664825439453 and loss=29.7383975982666\n",
      "The parameters are w1=1.648761510848999, w2=-0.7301762700080872 b=0.354964941740036 and loss=15.808223724365234\n",
      "The parameters are w1=1.651724934577942, w2=-0.7728831171989441 b=0.3541203737258911 and loss=19.00275421142578\n",
      "The parameters are w1=1.6646578311920166, w2=-0.7892255783081055 b=0.356704443693161 and loss=1.1891162395477295\n",
      "The parameters are w1=1.7051005363464355, w2=-0.786437451839447 b=0.36412742733955383 and loss=25.853134155273438\n",
      "The parameters are w1=1.71338951587677, w2=-0.8076227903366089 b=0.36572304368019104 and loss=1.6811765432357788\n",
      "The parameters are w1=1.719739317893982, w2=-0.8419188857078552 b=0.3658822178840637 and loss=14.44530200958252\n",
      "The parameters are w1=1.742104411125183, w2=-0.8477673530578613 b=0.37207162380218506 and loss=14.842279434204102\n",
      "The parameters are w1=1.749672532081604, w2=-0.8780274391174316 b=0.3726784586906433 and loss=12.121187210083008\n",
      "The parameters are w1=1.7697787284851074, w2=-0.8808921575546265 b=0.37701496481895447 and loss=0.47911137342453003\n",
      "The parameters are w1=1.7782261371612549, w2=-0.9083771109580994 b=0.3779025077819824 and loss=10.42898178100586\n",
      "The parameters are w1=1.7937414646148682, w2=-0.9162997603416443 b=0.3812885880470276 and loss=0.5259701609611511\n",
      "The parameters are w1=1.8090667724609375, w2=-0.9242939949035645 b=0.38465824723243713 and loss=0.5855758786201477\n",
      "The parameters are w1=1.8451443910598755, w2=-0.9193601608276367 b=0.39154356718063354 and loss=18.95964241027832\n",
      "The parameters are w1=1.853790283203125, w2=-0.9355019330978394 b=0.39337942004203796 and loss=1.0429683923721313\n",
      "The parameters are w1=1.864760160446167, w2=-0.9479063153266907 b=0.3958221971988678 and loss=0.9449997544288635\n",
      "The parameters are w1=1.897042155265808, w2=-0.945114254951477 b=0.4019964039325714 and loss=15.787403106689453\n",
      "The parameters are w1=1.9044584035873413, w2=-0.9610836505889893 b=0.4036780595779419 and loss=1.3501478433609009\n",
      "The parameters are w1=1.9121052026748657, w2=-0.9759926795959473 b=0.4054407775402069 and loss=1.1127448081970215\n",
      "The parameters are w1=1.917945146560669, w2=-1.0008496046066284 b=0.4060523808002472 and loss=7.5291266441345215\n",
      "The parameters are w1=1.933957815170288, w2=-1.003839135169983 b=0.40966448187828064 and loss=0.3328777849674225\n",
      "The parameters are w1=1.9630417823791504, w2=-1.0015751123428345 b=0.41532206535339355 and loss=12.957332611083984\n",
      "The parameters are w1=1.9699612855911255, w2=-1.015976071357727 b=0.4169917702674866 and loss=1.1874850988388062\n",
      "The parameters are w1=1.983127474784851, w2=-1.0217071771621704 b=0.42001134157180786 and loss=0.15623171627521515\n",
      "The parameters are w1=1.9987136125564575, w2=-1.0270131826400757 b=0.4248606562614441 and loss=10.883145332336426\n",
      "The parameters are w1=2.003626823425293, w2=-1.0429894924163818 b=0.426079660654068 and loss=1.1455204486846924\n",
      "The parameters are w1=2.010632038116455, w2=-1.0553144216537476 b=0.4278407692909241 and loss=0.9760527610778809\n",
      "The parameters are w1=2.0360403060913086, w2=-1.0538113117218018 b=0.43288514018058777 and loss=9.823066711425781\n",
      "The parameters are w1=2.0494489669799805, w2=-1.0602476596832275 b=0.4372609257698059 and loss=10.134916305541992\n",
      "The parameters are w1=2.0632317066192627, w2=-1.0663306713104248 b=0.44169357419013977 and loss=9.900107383728027\n",
      "The parameters are w1=2.076258897781372, w2=-1.0729938745498657 b=0.44595545530319214 and loss=9.688203811645508\n",
      "The parameters are w1=2.095008373260498, w2=-1.076947808265686 b=0.4496716856956482 and loss=7.1026482582092285\n",
      "The parameters are w1=2.094411611557007, w2=-1.1033803224563599 b=0.4491642415523529 and loss=5.44157075881958\n",
      "The parameters are w1=2.107665777206421, w2=-1.1080495119094849 b=0.45351964235305786 and loss=9.376014709472656\n",
      "The parameters are w1=2.111271858215332, w2=-1.1217942237854004 b=0.45464950799942017 and loss=1.2001537084579468\n",
      "The parameters are w1=2.116124153137207, w2=-1.133536696434021 b=0.45607784390449524 and loss=1.0030039548873901\n",
      "The parameters are w1=2.129427194595337, w2=-1.1369092464447021 b=0.4605148732662201 and loss=9.187118530273438\n",
      "The parameters are w1=2.133483409881592, w2=-1.1489018201828003 b=0.46179309487342834 and loss=0.9534923434257507\n",
      "The parameters are w1=2.139134407043457, w2=-1.158743143081665 b=0.46344029903411865 and loss=0.8310186862945557\n",
      "The parameters are w1=2.1494855880737305, w2=-1.161605715751648 b=0.46611151099205017 and loss=0.14188264310359955\n",
      "The parameters are w1=2.168555974960327, w2=-1.1609662771224976 b=0.47012823820114136 and loss=5.468532085418701\n",
      "The parameters are w1=2.1763863563537598, w2=-1.1666250228881836 b=0.4722152054309845 and loss=0.057536251842975616\n",
      "The parameters are w1=2.187969923019409, w2=-1.17075514793396 b=0.4762563705444336 and loss=8.34831714630127\n",
      "The parameters are w1=2.1988213062286377, w2=-1.1755201816558838 b=0.48012983798980713 and loss=8.17706298828125\n",
      "The parameters are w1=2.2129671573638916, w2=-1.178921103477478 b=0.4831593632698059 and loss=3.9484236240386963\n",
      "The parameters are w1=2.221376657485962, w2=-1.1857335567474365 b=0.48650047183036804 and loss=7.8427205085754395\n",
      "The parameters are w1=2.2337872982025146, w2=-1.1902674436569214 b=0.4891970455646515 and loss=3.3713133335113525\n",
      "The parameters are w1=2.237679958343506, w2=-1.199173092842102 b=0.49045565724372864 and loss=0.007129264529794455\n",
      "The parameters are w1=2.237043857574463, w2=-1.2144311666488647 b=0.4907723367214203 and loss=1.2611451148986816\n",
      "The parameters are w1=2.2395107746124268, w2=-1.229205846786499 b=0.49139344692230225 and loss=2.1156082153320312\n",
      "The parameters are w1=2.2470433712005615, w2=-1.2324992418289185 b=0.49355703592300415 and loss=0.07788822799921036\n",
      "The parameters are w1=2.2500784397125244, w2=-1.241963505744934 b=0.49477314949035645 and loss=0.8417038321495056\n",
      "The parameters are w1=2.254408121109009, w2=-1.2528687715530396 b=0.49591299891471863 and loss=1.4932336807250977\n",
      "The parameters are w1=2.2586112022399902, w2=-1.2600539922714233 b=0.49742215871810913 and loss=0.6117492914199829\n",
      "The parameters are w1=2.263610601425171, w2=-1.26883864402771 b=0.4988052248954773 and loss=1.1247453689575195\n",
      "The parameters are w1=2.279034376144409, w2=-1.266884684562683 b=0.5023452043533325 and loss=3.0652828216552734\n",
      "The parameters are w1=2.292773962020874, w2=-1.2662218809127808 b=0.5055439472198486 and loss=2.606024980545044\n",
      "The parameters are w1=2.2987236976623535, w2=-1.2699165344238281 b=0.5074105858802795 and loss=0.05170222744345665\n",
      "The parameters are w1=2.308227300643921, w2=-1.2720917463302612 b=0.5110834240913391 and loss=6.971054553985596\n",
      "The parameters are w1=2.3167190551757812, w2=-1.275127649307251 b=0.5145239233970642 and loss=6.84024715423584\n",
      "The parameters are w1=2.321188449859619, w2=-1.2801285982131958 b=0.5160694122314453 and loss=0.023750657215714455\n",
      "The parameters are w1=2.3211073875427246, w2=-1.2914462089538574 b=0.5166525840759277 and loss=0.9378567934036255\n",
      "The parameters are w1=2.3267815113067627, w2=-1.294469952583313 b=0.5185035467147827 and loss=0.05131196230649948\n",
      "The parameters are w1=2.32816481590271, w2=-1.3032184839248657 b=0.5194616317749023 and loss=0.8005020618438721\n",
      "y value for x1 = 3 and x2 = 2 =3.03775691986084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,X1,X2,Y):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X1)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X1[idx],self.X2[idx],self.Y[idx]\n",
    "\n",
    "x1 = torch.tensor([3,4,5,6,2])\n",
    "x2 = torch.tensor([8,5,7,3,1])\n",
    "y = torch.tensor([-3.5,3.5,2.5,11.5,5.7])\n",
    "\n",
    "dataset = MyDataset(x1,x2,y)\n",
    "data_loader = DataLoader(dataset,batch_size=2,shuffle=True)\n",
    "\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        self.w2 = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        self.b = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        \n",
    "    def forward(self,x1,x2):\n",
    "        return self.w1*x1 + self.w2*x2 + self.b\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "model = RegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001)\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "    for i,data in enumerate(data_loader):\n",
    "        x1,x2,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x1,x2)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss += loss.item()\n",
    "\n",
    "    print(\"The parameters are w1={}, w2={} b={} and loss={}\".format(model.w1.item(), model.w2.item(),model.b.item(), loss.item()))\n",
    "\n",
    "\n",
    "y_result = model.w1.item()*x1 + model.w2.item()*x2 + model.b.item()\n",
    "print(\"y value for x1 = 3 and x2 = 2 = {}\".format(y_result.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7559f-3bb7-46a6-9a72-4f4eb0f322cf",
   "metadata": {},
   "source": [
    " ## Question 7 - Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79175a24-96c9-49bf-9db3-8ff46ed0eb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=0.41448289155960083, b=0.06592708081007004 and loss=0.0\n",
      "The parameters are w=0.3647388815879822, b=0.06145741790533066 and loss=19.61363983154297\n",
      "The parameters are w=0.315403014421463, b=0.05704847350716591 and loss=0.0\n",
      "The parameters are w=0.2668446898460388, b=0.05276854708790779 and loss=3.0994440294307424e-06\n",
      "The parameters are w=0.21910151839256287, b=0.048581771552562714 and loss=4.848714351654053\n",
      "The parameters are w=0.1722472906112671, b=0.04449925944209099 and loss=0.00034800218418240547\n",
      "The parameters are w=0.12712997198104858, b=0.04061070457100868 and loss=3.2293167114257812\n",
      "The parameters are w=0.08510959148406982, b=0.03700099512934685 and loss=0.003284169128164649\n",
      "The parameters are w=0.047873370349407196, b=0.033651821315288544 and loss=1.470564603805542\n",
      "The parameters are w=0.02395070716738701, b=0.030726762488484383 and loss=2.773238182067871\n",
      "The parameters are w=0.03727197274565697, b=0.028490934520959854 and loss=1.9026623964309692\n",
      "The parameters are w=0.03918427601456642, b=0.02601717598736286 and loss=0.18442651629447937\n",
      "The parameters are w=0.030187280848622322, b=0.023434432223439217 and loss=1.808624505996704\n",
      "The parameters are w=0.019870499148964882, b=0.02077667973935604 and loss=2.585263967514038\n",
      "The parameters are w=0.03776994347572327, b=0.01862461306154728 and loss=1.4445420503616333\n",
      "The parameters are w=0.03366028890013695, b=0.016061147674918175 and loss=0.11591321229934692\n",
      "The parameters are w=0.017550969496369362, b=0.013342131860554218 and loss=2.4776930809020996\n",
      "The parameters are w=0.018232960253953934, b=0.010970963165163994 and loss=2.501673698425293\n",
      "The parameters are w=0.04141261801123619, b=0.008862696588039398 and loss=0.33872178196907043\n",
      "The parameters are w=0.039492540061473846, b=0.0063502853736281395 and loss=0.18387246131896973\n",
      "The parameters are w=0.032234203070402145, b=0.003721509827300906 and loss=0.17706668376922607\n",
      "The parameters are w=0.014308381825685501, b=0.0010603052796795964 and loss=1.4027382135391235\n",
      "The parameters are w=0.03889667987823486, b=-0.0010052246507257223 and loss=0.4101390242576599\n",
      "The parameters are w=0.022302549332380295, b=-0.0037511982955038548 and loss=2.64564847946167\n",
      "The parameters are w=0.043656643480062485, b=-0.0057562305592000484 and loss=1.4255928993225098\n",
      "The parameters are w=0.033716145902872086, b=-0.008397272787988186 and loss=0.41767066717147827\n",
      "The parameters are w=0.026466073468327522, b=-0.010871520265936852 and loss=1.7226554155349731\n",
      "The parameters are w=0.04654192179441452, b=-0.013133137486875057 and loss=0.09568239748477936\n",
      "The parameters are w=0.022098232060670853, b=-0.016016801819205284 and loss=2.6166629791259766\n",
      "The parameters are w=0.06319813430309296, b=-0.017932286486029625 and loss=0.025826068595051765\n",
      "The parameters are w=0.041593197733163834, b=-0.02085009030997753 and loss=0.15349717438220978\n",
      "The parameters are w=0.041237372905015945, b=-0.023249154910445213 and loss=0.15974126756191254\n",
      "The parameters are w=0.039982009679079056, b=-0.025691775605082512 and loss=0.3930005729198456\n",
      "The parameters are w=0.027534618973731995, b=-0.028337223455309868 and loss=1.71463942527771\n",
      "The parameters are w=0.03681698068976402, b=-0.030512088909745216 and loss=1.5609737634658813\n",
      "The parameters are w=0.034860290586948395, b=-0.03292423114180565 and loss=1.7969002723693848\n",
      "The parameters are w=0.03604988753795624, b=-0.035249013453722 and loss=0.3109396994113922\n",
      "The parameters are w=0.03988402709364891, b=-0.0376814641058445 and loss=1.852685570716858\n",
      "The parameters are w=0.03491515293717384, b=-0.04013149067759514 and loss=0.5928912162780762\n",
      "The parameters are w=0.035293541848659515, b=-0.042607974261045456 and loss=1.7904046773910522\n",
      "The parameters are w=0.03259485214948654, b=-0.04501930996775627 and loss=1.5224452018737793\n",
      "The parameters are w=0.04465403035283089, b=-0.047238439321517944 and loss=1.3847098350524902\n",
      "The parameters are w=0.03854746371507645, b=-0.049782201647758484 and loss=0.05854344740509987\n",
      "The parameters are w=0.03634059801697731, b=-0.05223620682954788 and loss=1.7914259433746338\n",
      "The parameters are w=0.025225814431905746, b=-0.054790198802948 and loss=1.6567813158035278\n",
      "The parameters are w=0.01387725118547678, b=-0.05727945268154144 and loss=1.4117251634597778\n",
      "The parameters are w=0.03672851249575615, b=-0.05941958725452423 and loss=1.5291790962219238\n",
      "The parameters are w=0.028745664283633232, b=-0.06197033450007439 and loss=2.819556951522827\n",
      "The parameters are w=0.03583640232682228, b=-0.0641808733344078 and loss=1.519278645515442\n",
      "The parameters are w=0.019680753350257874, b=-0.06682581454515457 and loss=1.579995036125183\n",
      "The parameters are w=0.03836993873119354, b=-0.0689140185713768 and loss=1.7958149909973145\n",
      "The parameters are w=0.03287022188305855, b=-0.07132180035114288 and loss=0.746464192867279\n",
      "The parameters are w=0.03555984050035477, b=-0.07376181334257126 and loss=1.7560622692108154\n",
      "The parameters are w=0.022863904014229774, b=-0.07634227722883224 and loss=1.605124592781067\n",
      "The parameters are w=0.018187211826443672, b=-0.07881558686494827 and loss=2.3546249866485596\n",
      "The parameters are w=0.03954248130321503, b=-0.08086217939853668 and loss=1.5212604999542236\n",
      "The parameters are w=0.03347652405500412, b=-0.08337552845478058 and loss=0.15500220656394958\n",
      "The parameters are w=0.030213912948966026, b=-0.0857742503285408 and loss=1.678702473640442\n",
      "The parameters are w=0.03452480211853981, b=-0.08803102374076843 and loss=0.6644437909126282\n",
      "The parameters are w=0.03657185658812523, b=-0.09044209122657776 and loss=0.264945387840271\n",
      "The parameters are w=0.03344631567597389, b=-0.09291960299015045 and loss=0.34238341450691223\n",
      "The parameters are w=0.03226368874311447, b=-0.0952070951461792 and loss=0.8243353962898254\n",
      "The parameters are w=0.03534858301281929, b=-0.09748921543359756 and loss=0.35510483384132385\n",
      "The parameters are w=0.035069093108177185, b=-0.09985155612230301 and loss=0.46955007314682007\n",
      "The parameters are w=0.04882412031292915, b=-0.1020536944270134 and loss=0.24100558459758759\n",
      "The parameters are w=0.04649931564927101, b=-0.10449937731027603 and loss=0.08373203128576279\n",
      "The parameters are w=0.04578134045004845, b=-0.10689039528369904 and loss=1.327059268951416\n",
      "The parameters are w=0.02680739015340805, b=-0.10959390550851822 and loss=1.306140422821045\n",
      "The parameters are w=0.03647478297352791, b=-0.11171727627515793 and loss=1.721757173538208\n",
      "The parameters are w=0.03691817820072174, b=-0.11409809440374374 and loss=0.263473778963089\n",
      "The parameters are w=0.03485266864299774, b=-0.11650282889604568 and loss=1.3072311878204346\n",
      "The parameters are w=0.04180337116122246, b=-0.11871995031833649 and loss=0.3941066563129425\n",
      "The parameters are w=0.02638844959437847, b=-0.12132678925991058 and loss=2.6199026107788086\n",
      "The parameters are w=0.03545185178518295, b=-0.1234683021903038 and loss=0.6520835161209106\n",
      "The parameters are w=0.04017244651913643, b=-0.12572665512561798 and loss=0.4486953616142273\n",
      "The parameters are w=0.035590726882219315, b=-0.12816806137561798 and loss=0.37417271733283997\n",
      "The parameters are w=0.04370517656207085, b=-0.1302269846200943 and loss=0.0337713249027729\n",
      "The parameters are w=0.0355539545416832, b=-0.13268856704235077 and loss=1.6862688064575195\n",
      "The parameters are w=0.040883440524339676, b=-0.1349051594734192 and loss=0.048086848109960556\n",
      "The parameters are w=0.037026941776275635, b=-0.1372738778591156 and loss=0.0833163931965828\n",
      "The parameters are w=0.034896913915872574, b=-0.13966652750968933 and loss=1.4350042343139648\n",
      "The parameters are w=0.046928003430366516, b=-0.14187772572040558 and loss=0.2864818871021271\n",
      "The parameters are w=0.03098609670996666, b=-0.1445140391588211 and loss=1.6199405193328857\n",
      "The parameters are w=0.04510170966386795, b=-0.14672185480594635 and loss=1.4808142185211182\n",
      "The parameters are w=0.028093427419662476, b=-0.14934132993221283 and loss=1.581800937652588\n",
      "The parameters are w=0.02914644591510296, b=-0.15165545046329498 and loss=2.684523820877075\n",
      "The parameters are w=0.018455928191542625, b=-0.15414941310882568 and loss=1.4710206985473633\n",
      "The parameters are w=0.03934389352798462, b=-0.15610846877098083 and loss=1.703237533569336\n",
      "The parameters are w=0.02036770060658455, b=-0.1586921066045761 and loss=1.3426408767700195\n",
      "The parameters are w=0.03180385380983353, b=-0.16078515350818634 and loss=1.6107416152954102\n",
      "The parameters are w=0.03971324488520622, b=-0.16298390924930573 and loss=0.49172455072402954\n",
      "The parameters are w=0.022833919152617455, b=-0.16559532284736633 and loss=2.4017510414123535\n",
      "The parameters are w=0.052283987402915955, b=-0.16759173572063446 and loss=0.21090613305568695\n",
      "The parameters are w=0.03525444492697716, b=-0.1702135056257248 and loss=1.2567557096481323\n",
      "The parameters are w=0.03678785637021065, b=-0.1724872589111328 and loss=1.2560646533966064\n",
      "The parameters are w=0.02752978727221489, b=-0.17492224276065826 and loss=1.547053337097168\n",
      "The parameters are w=0.038139708340168, b=-0.17702028155326843 and loss=1.4133274555206299\n",
      "The parameters are w=0.04695906490087509, b=-0.17911553382873535 and loss=0.11393951624631882\n",
      "The parameters are w=0.03370174393057823, b=-0.1816501021385193 and loss=1.2446715831756592\n",
      "The parameters are w=0.03634364530444145, b=-0.1839074194431305 and loss=0.4586004912853241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "x = torch.tensor([1,5,10,10,25,50,70,75,100])\n",
    "y = torch.tensor([0,0,0,0,0,1,1,1,1])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.Y[idx]\n",
    "\n",
    "dataset = MyDataset(x,y)\n",
    "data_loader = DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        self.b = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.w*x + self.b\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "model = RegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001)\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "    for i,data in enumerate(data_loader):\n",
    "        inputs,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        outputs = sigmoid(outputs)\n",
    "        labels = labels.to(torch.float32)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.item()\n",
    "    print(\"The parameters are w={}, b={} and loss={}\".format(model.w.item(), model.b.item(), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af777eb-f14f-44ec-8f86-17d6c572fb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
